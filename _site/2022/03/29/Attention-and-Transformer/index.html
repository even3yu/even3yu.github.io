<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>注意力机制和 Transformer (Attention and Transformer)</title>
    <meta name="description" content="Self-Attention 和 Transformer 自从问世就成为了自然语言处理领域的新星. 得益于全局的注意力机制和并行化的训练, 基于 Transformer 的自然语言模型能够方便的编码的长距离依赖关系, 同时在大规模自然语言数据集上并行训练成为可能.">

    <!-- 网站所有权验证 -->
    <meta name="baidu-site-verification" content="code-kzX4R1yDEi" />
    <meta name="google-site-verification" content="kj0sMKl0iZFsV2KPqmN9OJ3S7aeCrJnNYAOTpJzXCz4" />
    <meta name="msvalidate.01" content="C9A829578EE81A43ECA102B601A5E052" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/css/zui.min.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2022/03/29/Attention-and-Transformer/">
    <link rel="alternate" type="application/rss+xml" title="Jarvis' Blog (总有美丽的风景让人流连)" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?b9d127980a49e998bbedb8aab536a81d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108096001-2', 'auto');
      ga('send', 'pageview');

    </script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [["$$ "," $$"],["\\(","\\)"]],
    processEscapes: true,
    tags: "all",
    macros: {
      bm: ["{\\boldsymbol #1}",1]
    },
    packages: {'[+]': ['noerrors']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['input/asciimath', '[tex]/noerrors']
  }
};
</script>
<script async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" id="MathJax-script">
</script>
<!-- src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> -->



<!--  -->
    
<script type="text/javascript">
    var host = "jarvis73.com";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
      window.location.protocol = "https";
</script>
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/index.html" class="brand">Jarvis' Blog (总有美丽的风景让人流连)</a>
        <small>总有美丽的风景让人流连</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/index.html">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/wiki/">
                        
                            <i class="fa fa-book"></i>Wiki
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left-jekyll">
        <h1>注意力机制和 Transformer (Attention and Transformer)</h1>
        <div class="label-custom">

            <div class="label-custom-card">
                <i class="fa fa-calendar"></i>2022-03-29
            </div>

            

            <div class="label-custom-card">
                <i class="fa fa-user"></i>Jarvis
                
            </div>

            <div class="label-custom-card">
                <i class="fa fa-key"></i>Post  
            </div>

            <div class="label-custom-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#表示学习" title="Category: 表示学习" rel="category">表示学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#1-基本概念" id="markdown-toc-1-基本概念">1. 基本概念</a>    <ul>
      <li><a href="#11-self-attention" id="markdown-toc-11-self-attention">1.1 Self-Attention</a></li>
      <li><a href="#12-transformer" id="markdown-toc-12-transformer">1.2 Transformer</a></li>
    </ul>
  </li>
  <li><a href="#2-vision-transformer-vit" id="markdown-toc-2-vision-transformer-vit">2. Vision-Transformer (ViT)</a>    <ul>
      <li><a href="#21-模型" id="markdown-toc-21-模型">2.1 模型</a></li>
      <li><a href="#22-数据集" id="markdown-toc-22-数据集">2.2 数据集</a></li>
      <li><a href="#23-结构和参数" id="markdown-toc-23-结构和参数">2.3 结构和参数</a>        <ul>
          <li><a href="#参数量计算" id="markdown-toc-参数量计算">参数量计算</a></li>
        </ul>
      </li>
      <li><a href="#24-结果" id="markdown-toc-24-结果">2.4 结果</a></li>
    </ul>
  </li>
  <li><a href="#3-data-efficient-image-transformer-deit" id="markdown-toc-3-data-efficient-image-transformer-deit">3. Data-efficient image Transformer (DeiT)</a>    <ul>
      <li><a href="#31-模型" id="markdown-toc-31-模型">3.1 模型</a></li>
      <li><a href="#32-结构" id="markdown-toc-32-结构">3.2 结构</a></li>
      <li><a href="#33-结果" id="markdown-toc-33-结果">3.3 结果</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<p>Self-Attention 和 Transformer 自从问世就成为了自然语言处理领域的新星. 得益于全局的注意力机制和并行化的训练, 基于 Transformer 的自然语言模型能够方便的编码的长距离依赖关系, 同时在大规模自然语言数据集上并行训练成为可能.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/transformers.jpg" data-caption="" />
    
</div>

<h2 id="1-基本概念">1. 基本概念</h2>

<p>Self-Attention 和 Transformer<sup id="fnref:Transformer" role="doc-noteref"><a href="#fn:Transformer" class="footnote" rel="footnote">1</a></sup> 自从问世就成为了自然语言处理 (Natural Language Processing, NLP) 领域的新星. 得益于全局的注意力机制和并行化的训练, 基于 Transformer 的自然语言模型能够方便的编码的长距离依赖关系, 同时在大规模自然语言数据集上并行训练成为可能.</p>

<h3 id="11-self-attention">1.1 Self-Attention</h3>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/self-attention.png" data-caption="" />
    
</div>

<p>Self-Attention 在空间维度做全局吸收. 假设输入特征 \(F\in\mathbb{R}^{b\times l\times d_0}\), 其中 \(b\) 是批大小 (batch size), \(l\) 是空间维度 (NLP 中是序列长度, CV 中是图像长宽的乘积 \(h\times w\)), \(d_0\) 是特征维度.</p>

<ol>
  <li>首先对 \(F\) 做线性变化得到 \(Q=FW_Q, K=FW_K, V=FW_V \in\mathbb{R}^{b\times l\times d}\)</li>
  <li>计算 \(Q, K\) 的相似度矩阵 \(S = QK^T \in\mathbb{R}^{b\times l\times l}\)</li>
  <li>缩放相似度矩阵 \(S / \sqrt{d}\)</li>
  <li>Softmax 沿 \(K\) 的轴归一化得到权重 \(W=\text{Softmax}(S / \sqrt{d})\)</li>
  <li>对 \(V\) 加权求和得到新的特征</li>
</ol>

\[\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d}})V\]

<p>上面的过程称为 “Scaled Dot-Product Attention”. 如果把 \(Q, K, V\) 的通道划分为 \(n\) 份, 对每一份单独做 Self-Attention, 那么称为 Multi-Head Self-Attention (MSA).</p>

<p>注解:</p>
<ol>
  <li>Self-Attention 的 \(Q, K, V\) 来自同一个输入特征 \(F\). 由于矩阵化计算, 输入序列(图像)的空间点是并行处理的, 因此模型无法捕获序列的顺序信息和图像的空间位置信息. 所以需要对 \(Q, K, V\) 不同的空间点加上不同的位置编码 (positional encoding). 为了清晰, 上面的公式略去了位置编码.</li>
  <li>Cross-Attention 的 \(Q\) 来自输入特征 \(F_1\), 而 \(K, V\) 来自输入特征 \(F_2\)</li>
  <li>缩放. 当 \(C_1\) 较大时, 相似度矩阵在 Softmax 后梯度会偏小, 导致学习困难. <a href="/2021/06/04/figures/#softmax-%E7%9A%84%E6%B8%A9%E5%BA%A6%E7%B3%BB%E6%95%B0">见图</a>, Softmax 的梯度在 \(x\) 绝对值增大时梯度迅速衰减.</li>
  <li>加权. 根据当前点与其他点的相似度, 使用 \(V\) 重构当前点的信息.</li>
  <li>加入残差后 \(F_{out} = F_{in} + \text{Attention}(F_{in})\), 那么注意力学习的就是输出和输出的变化:
    <ul>
      <li>\(\text{Self-Attention}(F_{in}) = F_{out} - F_{in}\), 这可以看作输入特征吸收了自己其他位置的信息</li>
      <li>\(\text{Cross-Attention}(F_{in}^{(q)}, F_{in}^{(k)}) = F_{out}^{(q)} - F_{in}^{(q)}\), 这可以看作输入特征 \(F_{in_q}\) 吸收了 \(F_{in_k}\) 的信息.</li>
    </ul>
  </li>
</ol>

<h3 id="12-transformer">1.2 Transformer</h3>

<div class="polaroid-tiny">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/transformer.png" data-caption="" />
    
</div>

<p>现代卷积网络使用卷积作为基础模块, 多层卷积同时包含了空间维度和通道维度的信息交互.</p>

<p>而上面的 Attention 是空间维度上的信息交互, 因此为了更强的表达能力, 后面接一个前馈网络 (Feed Forward Network, FFN) 作用于每个(空间)点上, 实现通道维度的信息交互.</p>

<p>把上面的 Attention 和 FFN 作为一个完整的模块, 叠加多次, 就形成了一个编码器 (Encoder).</p>

<p>根据 NLP 任务的特点, 对输入语句利用编码器编码为一组固定的特征. 然后利用 Attention + FFN 的组合叠加多层构造解码器. 解码器不能并行输出整个输出序列, 因为输出序列中后面的词应当依赖前面的词产生. 给定输出序列的第一个提示词, 使用 Self-Attention 对自己进行编码, 再使用 Cross-Attention 吸收编码器给出的输入特征预测下一个词.</p>

<h2 id="2-vision-transformer-vit">2. Vision-Transformer (ViT)</h2>

<h3 id="21-模型">2.1 模型</h3>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/vit.png" data-caption="" />
    
</div>

<p>ViT<sup id="fnref:vit" role="doc-noteref"><a href="#fn:vit" class="footnote" rel="footnote">2</a></sup> 创造性地把图像分类问题看成序列分类问题.</p>

<p>首先把输入图像分成大小为 \(P\times P\) 的块 (比如 \(P = 16\)), 那么一个 \(C\times H\times W\) 的输入图像就可以划分成 \(N = HW / P^2\) 个大小为 \(C\times P\times P\) 的图像块 \(\mathbf{x}_p\in\mathbb{R}^{N\times (P^2\cdot C)}\).</p>

<p>然后使用一个线性变换 \(\mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D}\) 把每个图像块映射为一个 \(D\) 维的特征向量. 在输入序列开头增加一个 <code class="language-plaintext highlighter-rouge">[CLASS]</code> 的可学习的 token, 用于后续预测类别输出. 此外还需要加上位置编码以引入空间信息.</p>

<p>然后这个长度为 \(N + 1\) 的序列输入 Transformer 的编码器. 编码器由 MSA 和 FFN 交替组合构成, 每个 MSA 和 FFN 前都加入 Layer Norm, 后面都是用残差连接.</p>

<p>ViT 不需要解码器.</p>

<p>输出层只需要把开头的 <code class="language-plaintext highlighter-rouge">[CLASS]</code> token 取出来, 经过一个线性层后作为分类输出.</p>

<p>ViT 可以用公式表示为:</p>

\[\begin{align}
\mathbf{z}_0 &amp;= [\mathbf{x}_{\text{class}}; \mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E};] + \mathbf{E}_{\text{pos}}, &amp; \mathbf{E}&amp;\in\mathbb{R}^{(P^2\cdot C)\times D}, \mathbf{E}_{\text{pos}}\in\mathbb{R}^{(N + 1)\times D}, \\
\mathbf{z}'_l &amp;= \text{MSA}(\text{LN}(\mathbf{z}_{l-1})) + \mathbf{z}_{l-1}, &amp; l &amp;=1\dots L, \\
\mathbf{z}_l &amp;= \text{FFN}(\text{LN}(\mathbf{z}'_l)) + \mathbf{z}'_l,  &amp; l &amp;=1\dots L, \\
\mathbf{y} &amp;= \text{LN}(\mathbf{z}_L^0)
\end{align}\]

<h3 id="22-数据集">2.2 数据集</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">数据集</th>
      <th style="text-align: center">类别数量</th>
      <th style="text-align: center">图像数量</th>
      <th style="text-align: center">分辨率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">ImageNet</td>
      <td style="text-align: center">1k</td>
      <td style="text-align: center">1.3M</td>
      <td style="text-align: center">avg 469 x 387</td>
    </tr>
    <tr>
      <td style="text-align: center">ImageNet</td>
      <td style="text-align: center">21k</td>
      <td style="text-align: center">14M</td>
      <td style="text-align: center">avg 469 x 387</td>
    </tr>
    <tr>
      <td style="text-align: center">ImageNet Real</td>
      <td style="text-align: center">21k</td>
      <td style="text-align: center">14M</td>
      <td style="text-align: center">avg 469 x 387</td>
    </tr>
    <tr>
      <td style="text-align: center">JFT</td>
      <td style="text-align: center">18k</td>
      <td style="text-align: center">303M</td>
      <td style="text-align: center">-</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>ImageNet Real<sup id="fnref:IN-Real" role="doc-noteref"><a href="#fn:IN-Real" class="footnote" rel="footnote">3</a></sup> 是对 ImageNet 的标签进行修正后的数据集</li>
</ul>

<h3 id="23-结构和参数">2.3 结构和参数</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">模型</th>
      <th style="text-align: center">层数</th>
      <th style="text-align: center">输入序列维度 \(D\)</th>
      <th style="text-align: center">MLP 隐藏层维度</th>
      <th style="text-align: center">Head 数</th>
      <th style="text-align: center">参数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>ViT-S</strong>mall</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">384</td>
      <td style="text-align: center">1536</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">22M</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>ViT-B</strong>ase</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">768</td>
      <td style="text-align: center">3072</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">86M</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>ViT-L</strong>arge</td>
      <td style="text-align: center">24</td>
      <td style="text-align: center">1024</td>
      <td style="text-align: center">4096</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">307M</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>ViT-H</strong>uge</td>
      <td style="text-align: center">32</td>
      <td style="text-align: center">1280</td>
      <td style="text-align: center">5120</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">632M</td>
    </tr>
  </tbody>
</table>

<h4 id="参数量计算">参数量计算</h4>

<p>ViT 的参数依次包含在以下层:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">网络层</th>
      <th style="text-align: left">参数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Input Embedding Layer (input dim \(P^2\cdot C\), embedding dim \(D\))</td>
      <td style="text-align: left">\(P^2CD + D\)</td>
    </tr>
    <tr>
      <td style="text-align: right">Class Token</td>
      <td style="text-align: left">\(D\)</td>
    </tr>
    <tr>
      <td style="text-align: right">Positional Embedding (number patches \(N\))</td>
      <td style="text-align: left">\((N + 1)D\)</td>
    </tr>
    <tr>
      <td style="text-align: right">Attention (layers \(L\), heads \(H\))</td>
      <td style="text-align: left">\(L(4D^2 + 4D)\)</td>
    </tr>
    <tr>
      <td style="text-align: right">MLP (MLP hidden dim \(D'=4D\))</td>
      <td style="text-align: left">\(L(2DD' + D' + D)\)</td>
    </tr>
    <tr>
      <td style="text-align: right">Layer Norm</td>
      <td style="text-align: left">\(L\cdot4D + 2D\)</td>
    </tr>
    <tr>
      <td style="text-align: right">Head</td>
      <td style="text-align: left">?</td>
    </tr>
  </tbody>
</table>

<p>总的参数量为</p>

\[\text{#Transformer} = (P^2C + N + 5)D + L(12D^2 + 13D)\]

<h3 id="24-结果">2.4 结果</h3>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/vit_res.png" data-caption="" />
    
</div>

<div class="polaroid-small">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/vit_pretrain.png" data-caption="" />
    
</div>

<p>备注:</p>
<ul>
  <li>输入的图像序列也可以替换为卷积网络的特征图. 小模型使用特征图更好一些, 大模型两种输入序列的差距基本消失.</li>
  <li>ViT 可以达到和卷积网络相似的性能, 同时训练速度更快.</li>
  <li>ViT 在更大规模的数据集上 (JFT) 上的表现最好, 在稍小规模的数据集上 (ImageNet) 表现不如卷积网络.</li>
</ul>

<h2 id="3-data-efficient-image-transformer-deit">3. Data-efficient image Transformer (DeiT)</h2>

<h3 id="31-模型">3.1 模型</h3>

<p>ViT 需要使用大量的数据 (如 JFT) 先预训练, 然后在 ImageNet 上微调才能达到和卷积网络相同的性能.</p>

<div class="polaroid-tiny">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/deit.png" data-caption="" />
    
</div>

<p>DeiT 引入蒸馏来解决该问题. 如上图, DeiT 额外增加一个 distillation token, 并用该 token 对应的输出计算蒸馏损失. 还提出了 Hard Distillation 的方法. 即使用 teacher 的预测结果作为标签, 直接使用交叉熵训练 (不使用 KL divergence):</p>

\[\mathcal{L}^{\text{hardDistill}} = \frac12\mathcal{L}_{\text{CE}}(\phi(Z_s), y) + \frac12\mathcal{L}_{\text{CE}}(\phi(Z_s), y_t).\]

<h3 id="32-结构">3.2 结构</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">模型</th>
      <th style="text-align: center">层数</th>
      <th style="text-align: center">输入序列维度 \(D\)</th>
      <th style="text-align: center">Head 数</th>
      <th style="text-align: center">参数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>DeiT-Ti</strong>ny</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">192</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">5M</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>DeiT-S</strong>mall</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">384</td>
      <td style="text-align: center">6</td>
      <td style="text-align: center">22M</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>DeiT-B</strong>ase</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">768</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">86M</td>
    </tr>
  </tbody>
</table>

<h3 id="33-结果">3.3 结果</h3>

<p>DeiT 的主要结果如下</p>

<div class="polaroid-small">
    
    
    
    <img data-toggle="lightbox" src="/images/2022/03/deit_res.png" data-caption="" />
    
</div>

<p>备注:</p>
<ul>
  <li>下三行是使用了额外的 distillation token 的结果. 使用该 token 的效果更好.</li>
  <li>使用末尾 distillation embedding 做分类器的效果要好于开头 class embedding 做分类器.</li>
  <li>DeiT 分析了不同蒸馏标签的影响, 发现用 hard label 更好.</li>
  <li>使用 Convnet teacher 要好于使用 Transformer teacher. (见论文表2)</li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:Transformer" role="doc-endnote">

      <p><strong>Attention is all you need</strong> <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin <br />
<a href="https://arxiv.org/abs/1706.03762">[html]</a> in NeruIPS 2017. <a href="#fnref:Transformer" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vit" role="doc-endnote">

      <p><strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong> <br />
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al.<br />
<a href="http://arxiv.org/abs/2010.11929">[html]</a> in ICLR 2021. <a href="#fnref:vit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:IN-Real" role="doc-endnote">

      <p><strong>Are we done with ImageNet?</strong> <br />
Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, Aäron van den Oord<br />
<a href="https://arxiv.org/abs/2006.07159">[html]</a> in arXiv 2006. <a href="#fnref:IN-Real" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        </article>
        <hr>

        <!-- 
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
         -->

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2022/03/24/Power-Management/">Thinkpad X1C 睡眠耗电很快</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2022/04/20/Prompt-Learning/">Prompt 学习和微调 (Prompt Learning and Tuning)</a></p>
        
    </div>
</div>


        <!-- <h2 id="comments">Comments</h2>
        



 -->


    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right-jekyll">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id]):not([class])')
    for (var i = 0; i < aTags.length; i++) {
        if (aTags[i].getAttribute('href').startsWith('#'))
        {
            continue
        }
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>

    <footer class="site-footer">


    <div class="wrapper">

        <p class="contact">
            联系我: 
            <a href="https://github.com/jarvis73" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:zjw.math@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>   
            <a href="https://www.zhihu.com/people/lin-xi-1-1" title="Zhihu"><i class="iconfont icon-daoruzhihu"></i></a>      
        </p>
        <p>
            <i class="fa fa-eye" style="padding-right: 2px;"></i> 访问量: <span id="busuanzi_value_site_pv"></span>次 | <i class="fa fa-user" style="padding-right: 2px;"></i> 访客数<span id="busuanzi_value_site_uv"></span>人次
        </p>
        <p class="power">
            网站支持 <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a> | 主题支持 <a href="https://github.com/Gaohaoyang/gaohaoyang.github.io">HyG</a> & <a href="https://github.com/Jarvis73/jarvis73.github.io">Jarvis73</a>
        </p>
        <p>
            <img src="/images/misc/beian.png" style="padding-right: 2px; padding-bottom: 4px; vertical-align: middle;" /> <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo" class="beian" >浙公网安备 33010602011353号 | </a>
            <a target="_blank" href="https://beian.miit.gov.cn/" class="beian">浙ICP备2020038513号-1</a>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script type='text/javascript' src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>
    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/lib/jquery/jquery.js" charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/js/zui.min.js" charset="utf-8"></script>
    <script src="/js/index_page.js" charset="utf-8"></script>
    <script src="/js/functions.js" charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
  </body>

</html>
