<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>小样本学习: 基础 (Few-Shot Learning: Basic)</title>
    <meta name="description" content="">

    <!-- 网站所有权验证 -->
    <meta name="baidu-site-verification" content="code-kzX4R1yDEi" />
    <meta name="google-site-verification" content="kj0sMKl0iZFsV2KPqmN9OJ3S7aeCrJnNYAOTpJzXCz4" />
    <meta name="msvalidate.01" content="C9A829578EE81A43ECA102B601A5E052" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/css/zui.min.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2019/08/10/Few-Shot-Learning/">
    <link rel="alternate" type="application/rss+xml" title="Jarvis' Blog (总有美丽的风景让人流连)" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?b9d127980a49e998bbedb8aab536a81d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108096001-2', 'auto');
      ga('send', 'pageview');

    </script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [["$$ "," $$"],["\\(","\\)"]],
    processEscapes: true,
    tags: "all",
    macros: {
      bm: ["{\\boldsymbol #1}",1]
    },
    packages: {'[+]': ['noerrors']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['input/asciimath', '[tex]/noerrors']
  }
};
</script>
<script async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" id="MathJax-script">
</script>
<!-- src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> -->



<!--  -->
    
<script type="text/javascript">
    var host = "jarvis73.com";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
      window.location.protocol = "https";
</script>
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/index.html" class="brand">Jarvis' Blog (总有美丽的风景让人流连)</a>
        <small>总有美丽的风景让人流连</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/index.html">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/wiki/">
                        
                            <i class="fa fa-book"></i>Wiki
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left-jekyll">
        <h1>小样本学习: 基础 (Few-Shot Learning: Basic)</h1>
        <div class="label-custom">

            <div class="label-custom-card">
                <i class="fa fa-calendar"></i>2019-08-10
            </div>

            

            <div class="label-custom-card">
                <i class="fa fa-user"></i>Jarvis
                
            </div>

            <div class="label-custom-card">
                <i class="fa fa-key"></i>Post  
            </div>

            <div class="label-custom-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#少样本学习" title="Category: 少样本学习" rel="category">少样本学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#1-引言" id="markdown-toc-1-引言">1. 引言</a></li>
  <li><a href="#2-基本概念" id="markdown-toc-2-基本概念">2. 基本概念</a>    <ul>
      <li><a href="#21-问题定义" id="markdown-toc-21-问题定义">2.1 问题定义</a></li>
      <li><a href="#22-相关问题" id="markdown-toc-22-相关问题">2.2 相关问题</a></li>
      <li><a href="#23-核心问题" id="markdown-toc-23-核心问题">2.3 核心问题</a>        <ul>
          <li><a href="#231-经验风险最小化" id="markdown-toc-231-经验风险最小化">2.3.1 经验风险最小化</a></li>
          <li><a href="#232-不可靠的经验风险最小化" id="markdown-toc-232-不可靠的经验风险最小化">2.3.2 不可靠的经验风险最小化</a></li>
          <li><a href="#233-fsl-方法的分类" id="markdown-toc-233-fsl-方法的分类">2.3.3 FSL 方法的分类</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-基于数据的方法" id="markdown-toc-3-基于数据的方法">3. 基于数据的方法</a>    <ul>
      <li><a href="#31-训练集-dtrain-数据增广" id="markdown-toc-31-训练集-dtrain-数据增广">3.1 训练集 \(D^{train}\) 数据增广</a>        <ul>
          <li><a href="#311-人工规则" id="markdown-toc-311-人工规则">3.1.1 人工规则</a></li>
          <li><a href="#312-学习数据变换" id="markdown-toc-312-学习数据变换">3.1.2 学习数据变换</a></li>
        </ul>
      </li>
      <li><a href="#32-结合其他数据集的增广" id="markdown-toc-32-结合其他数据集的增广">3.2 结合其他数据集的增广</a></li>
    </ul>
  </li>
  <li><a href="#4-基于模型的方法" id="markdown-toc-4-基于模型的方法">4. 基于模型的方法</a>    <ul>
      <li><a href="#41-多任务学习-multitask-learning" id="markdown-toc-41-多任务学习-multitask-learning">4.1 多任务学习 Multitask Learning</a></li>
      <li><a href="#42-嵌入学习-embedding-learning" id="markdown-toc-42-嵌入学习-embedding-learning">4.2 嵌入学习 (Embedding Learning)</a></li>
      <li><a href="#43-外部记忆学习-learning-with-external-memory" id="markdown-toc-43-外部记忆学习-learning-with-external-memory">4.3 外部记忆学习 (Learning with External Memory)</a></li>
      <li><a href="#44-生成式建模-generative-modeling" id="markdown-toc-44-生成式建模-generative-modeling">4.4 生成式建模 (Generative Modeling)</a></li>
    </ul>
  </li>
  <li><a href="#5-算法-algorithm" id="markdown-toc-5-算法-algorithm">5. 算法 (Algorithm)</a></li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<blockquote>
  <p>深度神经网络在大数据上取得了骄人的成绩, 但在仅有少量样本时表现得不尽如人意. 为了解决该问题, 小样本学习(Few-Shot Learning, FSL)被越来越多的研究者所关注. FSL 可以在十分有限的监督信息(带标注的样本)下, 充分利用先验信息来提升模型的表现. 本文对 FSL 的做一个综述, 并指出了 FSL 方法要解决的核心问题是不可靠的经验风险最小化. 基于先验信息如何被用于解决该核心问题, FSL 方法可以划分为三种类型:</p>
  <ol>
    <li>数据增广</li>
    <li>缩小模型的假设空间</li>
    <li>设计更好的优化算法</li>
  </ol>
</blockquote>

<h2 id="1-引言">1. 引言</h2>

<p>为了能够从有限的监督信息中学习模型, 人们提出了小样本学习(Few-Shot Learning, FSL)的概念<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, 一旦某些类别已经被模型学到, 那么这个学习的过程中就会有信息被抽象出来, 使得该模型对其他类别的学习更有效. 当只有一个样本有监督信息时, FSL 也成为单样本学习(One-Shot Learning). FSL 可以通过结合先验信息从少量样本中学习新的任务.</p>

<p>FSL 有许多使用场景. (1) 用于测试 AI 程序是否能像人那样做出决策. (2) 帮助工业界减轻收集大规模训练数据的负担, 比如 ResNet<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 在 1000 类图像分类数据集上获得了人类水平的表现, 但该数据集包含了百万张图像及标注. (3) 因为某些原因, 数据集的监督信息难以获得的情况, 如医学图像的标注需要专家进行, 药品研发的过程中可能产生毒性等等. 这些因素使得收集大量的有标注数据时困难的.</p>

<p>本文主要内容来自以下文献:</p>

<ul>
  <li>Few-Shot Learning Survey<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></li>
</ul>

<h2 id="2-基本概念">2. 基本概念</h2>

<p>考虑有监督的学习任务 \(T\) , FSL 处理的数据集 \(D=\{D^{train}, D^{test}\}\) 中, \(D^{train}=\{(x^{(i)}, y^{(i)})\}_{i=1}^I\) 为训练集, 且 \(I\) 是很小的整数, \(D^{test}=\{x^{test}\}\) 为测试集. 通常, 我们考虑 <em>N-way-K-shot</em> 分类任务时, \(D^{train}\) 包含来自于 \(N\) 个类别, 每类 \(K\) 个共 \(I=KN\) 个样本. 令 \(p(x, y)\) 表示输入 \(x\) 和标签 \(y\) 的联合分布, \(\hat{h}\) 为从 \(x\) 映射到 \(y\) 的最优假设(即模型). FSL 即是要在训练集 \(D^{train}\) 上学习一个假设 \(\hat{h}\) , 并在测试集上评估假设的好坏. 通常根据先验知识我们可以选择一个假设空间 \(\mathcal{H}\) 来缩小寻找最优假设的范围, 同时把假设参数化以便于优化 \(h(\cdot;\theta)\) , 参数为 \(\theta\) . 优化算法则负责在 \(\mathcal{H}\) 中搜索对于 \(D^{train}\) 最优的 \(\theta\) . 通过模型的预测值 \(\hat{y}=h(x;\theta)\) 和真实结果 \(y\) 之间的损失函数 \(l(\hat{y}, y)\) 来衡量算法和模型的表现.</p>

<h3 id="21-问题定义">2.1 问题定义</h3>

<p>FSL 是机器学习的一个子领域, 因此我们首先回顾机器学习的定义.</p>

<blockquote>
  <p><strong>Definition 1. (Machine Learning<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup>)</strong> A computer program is said to <strong>learn</strong> from experience \(E\) with respect to some classes of task \(T\) and performance measure \(P\) if its performance can improve with \(E\) on \(T\) measured by \(P\) .</p>
</blockquote>

<p>所以一个机器学习问题由 \(E, T, P\) 三部分组成. 比如 (1) 图像识别任务(\(T\)), 我们希望得到高的分类准确率(\(P\)), 而训练模型使用的是大规模图像数据集(\(E\)) 如 ImageNet. (2) AlphaGo程序执行的下围棋的任务(\(T\)), 其目标是高胜率(\(P\)), 训练模型使用的是数千万场人类专家的下棋记录和大规模的自我博弈(\(E\)).</p>

<p>FSL 是一种特殊的机器学习任务, 这类任务只能使用少量的监督信息构成训练数据集. FSL 的定义如下:</p>

<blockquote>
  <p><strong>Definition 2. (Few-Shot Learning)</strong> A type of machine learning problem(specified by \(E, T\) and \(P\)) where \(E\) contains a little supervised information for the target \(T\).</p>
</blockquote>

<h3 id="22-相关问题">2.2 相关问题</h3>

<p>为了更好的了解 FSL, 这一小节列举一些和 FSL 相关的问题类别做比较.</p>

<ul>
  <li><strong><a href="https://minds.wisconsin.edu/handle/1793/60444">半监督学习(Semi-supervised Learning)</a></strong> 是同时从有标注和无标注的数据中学习最优假设 \(\hat{h}\) .  <strong><a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611972795.23">正样本半监督学习(Positive-unlabeled learning)</a></strong> 是一类特殊的半监督学习, 只有正样本和无标注样本可以使用. <strong><a href="https://minds.wisconsin.edu/handle/1793/60660">主动学习(active learning)</a></strong> 则由算法选择可以提供有用信息(informative)的无标注数据由专家给出标注, 反馈给模型学习. 所以 FSL 既可以是监督学习, 也可以是半监督学习, 取决于从有限的监督信息中可以获得哪种数据.</li>
  <li><strong><a href="https://www.computer.org/csdl/trans/tk/2009/09/ttk2009091263-abs.html">不平衡学习(Imbalance Learning)</a></strong> 从类别 \(y\) 的分布严重倾斜的数据集中学习. FSL 任务中每类数据本来就很少.</li>
  <li><strong><a href="https://ieeexplore.ieee.org/abstract/document/5288526/">迁移学习(Transfer Learning)</a></strong> 是把知识从数据量丰富的源域迁移到数据量不足的目标域. <strong><a href="http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation.pdf">领域自适应(domain adaption)</a></strong> 就是一种迁移学习问题. 另一种相关的迁移学习叫<strong><a href="https://arxiv.org/abs/1707.00600">零样本学习(zero-shot learning)</a></strong>, 它需要在已有类别中学习特征, 并根据已有特征的组合来判断新的从未见过的类别. 根据 FSL 的定义, FSL 不一定是迁移学习, 但当我们使用额外监督信息来提升目标数据集的表现时, 这本质上就是一种迁移学习, 只不过源域中的数据可能也不多.</li>
  <li><strong><a href="https://link.springer.com/chapter/10.1007/3-540-44668-0_13">元学习(Meta Learning)/学会学习(Learning to Learn)</a></strong> 通过提供的数据集和元学习器(meta-learner)从其他地方学到的知识在新的任务 \(T\) 上提升表现 \(P\) . 具体来说, 元学习器在许多任务中学习元信息(一般知识), 并能够使用任务相关的信息快速泛化到新的任务上. 许多 FSL 都是元学习方法, 使用元学习器作为先验信息.</li>
</ul>

<h3 id="23-核心问题">2.3 核心问题</h3>

<p>这一部分我们通过公式推导来看一下要解决 FSL 的问题, 应该从哪些方面着手.</p>

<p>通常机器学习中监督学习任务的目标函数写为:</p>

\[\min_{\theta}\sum_{(x^{(u)}, y^{(i)})\in D^{train}}l(h(x^{(i)};\theta), y),\]

<p>其中 \(l\) 表示损失函数, \(h\in\mathcal{H}\) 表示假设(即模型), \(\theta\) 为模型参数. 学习算法试图寻找最优的 \(\theta\) 来最小化该目标函数.</p>

<h4 id="231-经验风险最小化">2.3.1 经验风险最小化</h4>

<p>如果已经给定假设空间 \(\mathcal{H}\) ,任务 \(T\), 和数据分布 \(p(x, y)\), 那么我们当然希望选择最好的假设以最小化<strong>期望风险(expected risk)</strong> \(R\) , 定义为:</p>

\[R(h)=\int l(h(x), y)dp(x, y)=\mathbb{E}[l(h(x), y)].\]

<p>不幸的是绝大部分情况下我们并不知道数据分布 \(p(x, y)\), 我们往往只能获得一些数据样本 \(D^{train}=\{(x^{(i)}, y^{(i)})\}_i^I\) , 所以我们在这些数据<strong>独立同分布(i.i.d.)</strong>的假设下, 使用<strong>经验风险(empirical risk)</strong> \(R_I\) 来估计期望风险 \(R\). 经验风险定义为数据集 \(D^{train}\) 上 \(I\) 个样本损失的平均值:</p>

\[R_I(h)=\frac1n\sum_{i=1}^Il(h(x^{(i)}), y),\]

<p>而学习过程通过<strong>经验风险最小化(Empirical Risk Minimization, ERM)</strong>来完成.</p>

<p>下面我们对误差进行分解, 首先给出三种最优解的定义:</p>

<ul>
  <li>\(\hat{h}=\arg\min_fR(h)\), 表示 \(\hat{h}\) 是 \(R\) 的全局最优解</li>
  <li>\(h^\*=\arg\min_{h\in\mathcal{H}}R(h)\), 表示 \(h^*\) 是 \(R\) 在假设空间 \(\mathcal{H}\) 中的最优解</li>
  <li>\(h_I=\arg\min_{h\in\mathcal{H}}R_I(h)\), 表示 \(h_I\) 是 \(R_I\) 在假设空间 \(\mathcal{H}\) 中的最优解</li>
</ul>

<p>为了简便, 假设三种最优解都是唯一的. 对于随机的一个训练集, 某个学习任务上训练出来的模型的总误差可以表示为:</p>

\[\mathbb{E}[R(h_I)-R(\hat{h})]=\underbrace{\mathbb{E}[R(h^*)-R(\hat{h})]}_{\displaystyle\mathcal{E}_{app}(\mathcal{H})}+\underbrace{\mathbb{E}[R(h_I)-R(h^*)]}_{\displaystyle\mathcal{E}_{est}(\mathcal{H, I})},\]

<p>其中近似误差 \(\mathcal{E}_{app}(\mathcal{H})\) 衡量的是设置假设空间导致的误差, 即为模型误差; 而估计误差 \(\mathcal{E}_{est}(\mathcal{H, I})\) 则衡量的是使用经验误差代替期望误差产生的影响. 注意上述公式中没有优化算法的误差, 即我们暂且认为优化算法均能找到当前约束条件下的全局最优解.</p>

<p>总结下来可以发现影响总误差的由三个因素 (1) 假设空间 \(\mathcal{H}\) 的选择是否合理, 能否包含全局最优解, 我们称之为<strong>模型因素</strong> (2) 数据集 \(D^{train}\) 及其大小 \(I\), 称之为<strong>数据因素</strong> (3) 优化算法能否找到当前条件下的全局最优解, 称之为<strong>算法因素</strong>.</p>

<h4 id="232-不可靠的经验风险最小化">2.3.2 不可靠的经验风险最小化</h4>

<p>注意到数据因素导致的估计误差随着数据集的增大可以收敛到 \(0\) (如何证明?):</p>

\[\lim_{I\rightarrow\infty}\mathcal{E}_{est}(\mathcal{H, I})=\lim_{I\rightarrow\infty}\mathbb{E}[R(h_I)-R(h^*)]=0,\]

<p>这意味着数据集越大, 可以尽可能地降低估计误差的影响. 但这恰恰是 FSL 的短板——缺乏数据. 这使得 \(R_I(h)\) 对与 \(R(h^*)\) 的估计非常不准确, 这是 <strong>FSL 的核心问题: 经验风险最小值点 \(h_I\) 变得不可靠</strong>. 因此 FSL 比常规的学习任务变得更难. 下图很直观的表现了这种差别.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2019/08/pic-1.jpg" data-caption="大数据集和小数据集上的误差" />
    
        
        <div class="container">
            <p>图 1. 大数据集和小数据集上的误差</p>
        </div>
    
</div>

<h4 id="233-fsl-方法的分类">2.3.3 FSL 方法的分类</h4>

<p>FSL 必须使用先验信息来辅助任务的解决, 因此根据先验信息的不同使用方法可以分为三类:</p>

<table>
  <thead>
    <tr>
      <th>因素/类别</th>
      <th>先验信息使用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>数据</td>
      <td>使用先验信息增广数据集 \(D^{train}\) 从 \(I\) 个样本到 \(\tilde{I}\) 个样本 , 从而提高经验风险最小值点的准确性</td>
    </tr>
    <tr>
      <td>模型</td>
      <td>基于先验信息约束假设空间 \(\mathcal{H}\) 到更小的范围</td>
    </tr>
    <tr>
      <td>算法</td>
      <td>利用先验信息搜索出假设空间中使得假设 \(h^*\) 最优的参数 \(\theta\)</td>
    </tr>
  </tbody>
</table>

<p>三类先验信息的使用效果可以由以下示意图来反映:</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2019/08/pic-2.jpg" data-caption="FSL 方法从数据(左), 模型(中)和算法(右)三个方面来减小误差. 图(a)中数据集被增广到 $$ \tilde{I} $$ , 因此得到更大的数据集使得 ERM 可以得到更可靠的假设 $$ h_{\tilde{I}} $$ . 图(b)中的假设空间被先验信息约束到了更小的范围. 图(c)中根据先验信息优化策略有所改变." />
    
        
        <div class="container">
            <p>图 2. FSL 方法从数据(左), 模型(中)和算法(右)三个方面来减小误差. 图(a)中数据集被增广到 $$ \tilde{I} $$ , 因此得到更大的数据集使得 ERM 可以得到更可靠的假设 $$ h_{\tilde{I}} $$ . 图(b)中的假设空间被先验信息约束到了更小的范围. 图(c)中根据先验信息优化策略有所改变.</p>
        </div>
    
</div>

<h2 id="3-基于数据的方法">3. 基于数据的方法</h2>

<h3 id="31-训练集-dtrain-数据增广">3.1 训练集 \(D^{train}\) 数据增广</h3>

<h4 id="311-人工规则">3.1.1 人工规则</h4>

<p>基于人工规则的数据增广方法通常包括:</p>
<ul>
  <li>平移 (translating)</li>
  <li>翻转 (flipping)</li>
  <li>错切 (shearing)</li>
  <li>缩放 (scaling)</li>
  <li>镜像 (reflecting)</li>
  <li>裁剪 (cropping)</li>
  <li>旋转 (rotating)</li>
  <li>加噪 (noising)</li>
  <li>亮度 (birghtness)</li>
  <li>对比度 (contrast)</li>
  <li>饱和度 (saturation)</li>
</ul>

<p>(注: 后期增加各类框架的API)</p>

<h4 id="312-学习数据变换">3.1.2 学习数据变换</h4>

<p>基于学习的数据增广方法是含参的增广方法, 先验信息嵌入到增广模型的参数中.</p>

<h3 id="32-结合其他数据集的增广">3.2 结合其他数据集的增广</h3>

<h2 id="4-基于模型的方法">4. 基于模型的方法</h2>

<p>基于模型的方法又可以从建模策略分为四类:</p>

<table>
  <thead>
    <tr>
      <th>策略</th>
      <th>先验信息</th>
      <th>如何约束参数空间 \(\mathcal{H}\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>multitask learning</td>
      <td>其他任务 \(T\) 及相应的数据集 \(D\)</td>
      <td>参数共享</td>
    </tr>
    <tr>
      <td>embedding learning</td>
      <td>从/和其他任务 \(T\) 学到的嵌入</td>
      <td>特征降维, 在低维嵌入空间中分类</td>
    </tr>
    <tr>
      <td>learning with external memory</td>
      <td>从其他任务 \(T\) 学到的嵌入与记忆交互</td>
      <td> </td>
    </tr>
    <tr>
      <td>generative modeling</td>
      <td>从其他任务学到的先验模型</td>
      <td>限制分布的形式</td>
    </tr>
  </tbody>
</table>

<h3 id="41-多任务学习-multitask-learning">4.1 多任务学习 Multitask Learning</h3>

<p>多任务学习同时学习多个任务, 利用多任务的共同信息和每个人物的特有信息进行学习. 因此适用于 FSL. 当多任务学习处理不同域的数据集时, 也成为<strong>域适应 (domain adaption)</strong>. 多任务学习的多个任务的假设空间存在较强的关联, 这种关联可以通过共享参数来表示. 根据是否显式地约束参数空间, 多任务学习可以分为:</p>

<ul>
  <li><strong>硬 (Hard) 参数共享</strong>
    <ul>
      <li>共享神经网络前几层参数, 最后几层适配不同任务 (Fine-Grained Visual Categorization<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>).</li>
      <li>共享神经网络后几层参数, 前面的层针对不同源域和目标域学习不同的参数, 不同的域最终共享分类器 (Few-Shot Adversarial Domain Adaptation<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">6</a></sup>).</li>
      <li>共享某些层, 源任务更新共享层的参数和特有层的参数, 目标任务只更新其特有层的参数 (One-Shot Unsupervised Cross Domain Translation<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>).</li>
    </ul>
  </li>
  <li><strong>软 (Soft) 参数共享</strong>
软共享不进行参数的强约束, 只鼓励不同任务的参数尽可能满足某种条件 (比如使用正则化函数 \(L_1, L_2\) 约束不同任务参数的距离). 因此多任务参数软共享时, 不同任务仍然具有不同的假设空间和参数.</li>
</ul>

<h3 id="42-嵌入学习-embedding-learning">4.2 嵌入学习 (Embedding Learning)</h3>

<p>嵌入 (embedding) 在机器学习中通常表示数据点 \(x_i\in\mathcal{X}\subset\mathbb{R}^d\) 在一个低维特征空间中的特征映射 \(z_i\in\mathcal{Z}\subset\mathbb{R}^m,~m &lt; d\), 通常表示为一个向量. 嵌入学习则是考虑如何训练嵌入模型使得在特征空间中的各数据点的嵌入仍然保持原始的相似和不相似的关系. 嵌入函数的参数则需要从先验知识中学习, 并且可以考虑把任务有关 (task-specific) 的信息引入嵌入函数的参数中. 嵌入函数主要用于分类任务.</p>

<p>嵌入学习分类的依据是嵌入的相似性, 即计算测试数据 \(x^{test}\in D^{test}\) 和训练数据 \(x_i\in D^{train}\) 之间的相似性, 从而把相似度最高的那个训练样本的标签赋给测试数据. 嵌入学习有如下几个重要的组成部分:</p>
<ul>
  <li>测试数据的嵌入函数 \(f(\cdot)\)</li>
  <li>训练样本的嵌入函数 \(g(\cdot)\)</li>
  <li>度量 \(s(\cdot, \cdot)\)</li>
</ul>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2019/08/embedding_learning.png" data-caption="FSL 问题的嵌入学习图示" />
    
        
        <div class="container">
            <p>图 3. FSL 问题的嵌入学习图示</p>
        </div>
    
</div>

<p>最简单的一种思路就是在训练集上训练模型, 在测试集上测试. 但在小样本的前提下这样训练模型势必导致严重的过拟合. 因此在 FSL 中通常训练任务无关 (task-invariant) 的模型, 再泛化到测试集的类别上, 即元学习 (meta-learning) 的思路. 因此在这种思路下嵌入函数从其他任务中学习先验知识, 而嵌入函数的参数中<em>不明显包含</em>任务有关的信息, 即我们<em>一般不会</em>在少样本的训练集上训练模型的参数(尤其是在 \(D^{train}\) 特别小的时候, 如每类仅有一个有标签样本). 下文为了清晰, 我们把测试集上带标签的少样本数据集称为<strong>支撑集 (support sets)</strong>, 表示为 \(x_i\in D^{sup}\), 把用于训练任务无关模型的大规模数据集称为训练集, 该大规模训练集中包含标签, 但不包含测试集中的类别.</p>

<ul>
  <li>
    <p><strong>Matching Network</strong> 通过计算 \(f(x^{test})\) 和一系列 \(g(x_i), x_i\in D^{sup}\) 的相似度来对 \(x^{test}\) 进行分类, 其中 \(f(\cdot)\) 依赖于 \(D^{train}\) 而 \(g(\cdot)\) 使用双向 LSTM (biLSTM) 聚合了 \(D^{sup}\) 中所有样本的信息. 在 Matching Nets 中两个嵌入函数为不同的模型, 度量使用了余弦相似度 (cosine similarity).</p>
  </li>
  <li>
    <p><strong>Prototypical Network</strong> 对支撑集中每一类的样本计算一个原型 (prototype, \(c\)) 用来代表该类别的嵌入. 原型用该类别中有标签样本的嵌入的平均值表示, 即</p>

\[c_n = \frac1K\sum_{k=1}^K g(x_k^{(n)}),\]

    <p>其中 \(n\) 表示第 \(n\) 个类别. 在 Prototypical Nets 中, 两个嵌入函数使用了相同的模型, 度量使用了 \(L_2\) 距离.</p>
  </li>
  <li>
    <p><strong>Relative Representations</strong> 进一步使用模型来学习度量. 如 Relation Network 使用 CNN 首先把数据映射到嵌入空间, 然后把测试数据和支撑数据的嵌入并起来, 通过另一个神经网路预测相似度值.</p>
  </li>
  <li>
    <p><strong>Relation Graph</strong> 把支撑集和测试集的样本点作为图节点, 边权则通过学习得到. 测试集的分类任务利用邻近节点的信息进行预测.</p>
  </li>
</ul>

<h3 id="43-外部记忆学习-learning-with-external-memory">4.3 外部记忆学习 (Learning with External Memory)</h3>

<ol>
  <li>记忆检索相关的信息</li>
  <li>计算相似度</li>
  <li>记忆可以被更新</li>
</ol>

<p>如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2019/08/memory.png" data-caption="FSL 问题的嵌入学习图示" />
    
        
        <div class="container">
            <p>图 4. FSL 问题的嵌入学习图示</p>
        </div>
    
</div>

<h3 id="44-生成式建模-generative-modeling">4.4 生成式建模 (Generative Modeling)</h3>

<h2 id="5-算法-algorithm">5. 算法 (Algorithm)</h2>

<p>略</p>

<h2 id="参考文献">参考文献</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p><strong>One-Shot Learning of Object Categories</strong><br />
Li Fei-Fei, R. Fergus, and P. Perona. <br />
<a href="https://doi.org/10.1109/TPAMI.2006.79.">[link]</a>. In IEEE Transactions on Pattern Analysis and Machine Intelligence 28 (4): 594–611. 2006. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><strong>Deep Residual Learning for Image Recognition</strong><br />
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. <br />
<a href="https://arxiv.org/abs/1512.03385">[link]</a>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–78. 2016. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">

      <p><strong>Generalizing from a Few Examples: A Survey on Few-Shot Learning</strong> <br />
Wang, Yaqing, Quanming Yao, James Kwok, and Lionel M. Ni. <br />
<a href="http://arxiv.org/abs/1904.05046">[link]</a> In ArXiv:1904.05046 [Cs], April. 2019. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p><strong>Machine Learning Textbook</strong><br />
Tom Mitchell. <br />
<a href="http://www.cs.cmu.edu/~tom/mlbook.html">[link]</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">

      <p><strong>Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data</strong> <br />
Yabin Zhang, Hui Tang, Kui Jia <br />
<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yabin_Zhang_Fine-Grained_Visual_Categorization_ECCV_2018_paper.pdf">[link]</a> In ECCV. 2018: 233-248. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">

      <p><strong>One-shot Adversirial Domain Adaptation</strong> <br />
Saeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh,Gianfranco Doretto  <br />
<a href="http://papers.nips.cc/paper/7244-few-shot-adversarial-domain-adaptation">[link]</a> In NIPS 2017: 6670-6680. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">

      <p><strong>One-shot unsupervised cross domain translation</strong> <br />
Sagie Benaim, Lior Wolf <br />
<a href="https://papers.nips.cc/paper/7480-one-shot-unsupervised-cross-domain-translation.pdf">[link]</a> In NIPS 2018: 2104-2114. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        </article>
        <hr>

        <!-- 
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
         -->

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2019/07/29/GLCM/">灰度共生矩阵(Grey Level Co-occurrence Matrix, GLCM)</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2019/12/22/Tensorflow-Serving-Deploy/">Tensorflow 模型部署</a></p>
        
    </div>
</div>


        <!-- <h2 id="comments">Comments</h2>
        



 -->


    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right-jekyll">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id]):not([class])')
    for (var i = 0; i < aTags.length; i++) {
        if (aTags[i].getAttribute('href').startsWith('#'))
        {
            continue
        }
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>

    <footer class="site-footer">


    <div class="wrapper">

        <p class="contact">
            联系我: 
            <a href="https://github.com/jarvis73" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:zjw.math@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>   
            <a href="https://www.zhihu.com/people/lin-xi-1-1" title="Zhihu"><i class="iconfont icon-daoruzhihu"></i></a>      
        </p>
        <p>
            <i class="fa fa-eye" style="padding-right: 2px;"></i> 访问量: <span id="busuanzi_value_site_pv"></span>次 | <i class="fa fa-user" style="padding-right: 2px;"></i> 访客数<span id="busuanzi_value_site_uv"></span>人次
        </p>
        <p class="power">
            网站支持 <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a> | 主题支持 <a href="https://github.com/Gaohaoyang/gaohaoyang.github.io">HyG</a> & <a href="https://github.com/Jarvis73/jarvis73.github.io">Jarvis73</a>
        </p>
        <p>
            <img src="/images/misc/beian.png" style="padding-right: 2px; padding-bottom: 4px; vertical-align: middle;" /> <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo" class="beian" >浙公网安备 33010602011353号 | </a>
            <a target="_blank" href="https://beian.miit.gov.cn/" class="beian">浙ICP备2020038513号-1</a>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script type='text/javascript' src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>
    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/lib/jquery/jquery.js" charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/js/zui.min.js" charset="utf-8"></script>
    <script src="/js/index_page.js" charset="utf-8"></script>
    <script src="/js/functions.js" charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
  </body>

</html>
