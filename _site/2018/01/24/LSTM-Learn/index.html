<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>循环神经网络 (Recurrent Neural Network, RNN)</title>
    <meta name="description" content="">

    <!-- 网站所有权验证 -->
    <meta name="baidu-site-verification" content="code-kzX4R1yDEi" />
    <meta name="google-site-verification" content="kj0sMKl0iZFsV2KPqmN9OJ3S7aeCrJnNYAOTpJzXCz4" />
    <meta name="msvalidate.01" content="C9A829578EE81A43ECA102B601A5E052" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/css/zui.min.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2018/01/24/LSTM-Learn/">
    <link rel="alternate" type="application/rss+xml" title="Jarvis' Blog (总有美丽的风景让人流连)" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?b9d127980a49e998bbedb8aab536a81d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108096001-2', 'auto');
      ga('send', 'pageview');

    </script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [["$$ "," $$"],["\\(","\\)"]],
    processEscapes: true,
    tags: "all",
    macros: {
      bm: ["{\\boldsymbol #1}",1]
    },
    packages: {'[+]': ['noerrors']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['input/asciimath', '[tex]/noerrors']
  }
};
</script>
<script async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" id="MathJax-script">
</script>
<!-- src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> -->



<!--  -->
    
<script type="text/javascript">
    var host = "jarvis73.com";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
      window.location.protocol = "https";
</script>
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/index.html" class="brand">Jarvis' Blog (总有美丽的风景让人流连)</a>
        <small>总有美丽的风景让人流连</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/index.html">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/wiki/">
                        
                            <i class="fa fa-book"></i>Wiki
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left-jekyll">
        <h1>循环神经网络 (Recurrent Neural Network, RNN)</h1>
        <div class="label-custom">

            <div class="label-custom-card">
                <i class="fa fa-calendar"></i>2018-01-24
            </div>

            
            <div class="label-custom-card">
                <i class="fa fa-pencil"></i>2019-11-18
            </div>
            

            <div class="label-custom-card">
                <i class="fa fa-user"></i>Jarvis
                
            </div>

            <div class="label-custom-card">
                <i class="fa fa-key"></i>Post  
            </div>

            <div class="label-custom-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#深度学习" title="Category: 深度学习" rel="category">深度学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#1-循环神经网络-recurrent-neural-network-rnn" id="markdown-toc-1-循环神经网络-recurrent-neural-network-rnn">1. 循环神经网络 (Recurrent Neural Network, RNN)</a>    <ul>
      <li><a href="#11-rnn-的应用" id="markdown-toc-11-rnn-的应用">1.1 RNN 的应用</a></li>
      <li><a href="#12-rnn-的局限性" id="markdown-toc-12-rnn-的局限性">1.2 RNN 的局限性</a></li>
    </ul>
  </li>
  <li><a href="#2-长短期记忆-long-short-term-memory-lstm" id="markdown-toc-2-长短期记忆-long-short-term-memory-lstm">2. 长短期记忆 (Long Short Term Memory, LSTM)</a>    <ul>
      <li><a href="#21-lstm-的核心思想" id="markdown-toc-21-lstm-的核心思想">2.1 LSTM 的核心思想</a></li>
      <li><a href="#22-lstm-单元解析" id="markdown-toc-22-lstm-单元解析">2.2 LSTM 单元解析</a></li>
      <li><a href="#23-lstm-单元变体" id="markdown-toc-23-lstm-单元变体">2.3 LSTM 单元变体</a></li>
    </ul>
  </li>
  <li><a href="#3-门控循环单元gated-recurrent-unit-gru" id="markdown-toc-3-门控循环单元gated-recurrent-unit-gru">3. 门控循环单元(Gated Recurrent Unit, GRU)</a></li>
  <li><a href="#4-编码-解码器-encoder-decoder" id="markdown-toc-4-编码-解码器-encoder-decoder">4. 编码-解码器 (Encoder-Decoder)</a>    <ul>
      <li><a href="#41-编码-解码器模型的局限性" id="markdown-toc-41-编码-解码器模型的局限性">4.1 编码-解码器模型的局限性</a></li>
    </ul>
  </li>
  <li><a href="#5-注意力机制-attention-mechanism" id="markdown-toc-5-注意力机制-attention-mechanism">5. 注意力机制 (Attention Mechanism)</a>    <ul>
      <li><a href="#51-什么是注意力" id="markdown-toc-51-什么是注意力">5.1 什么是注意力?</a></li>
      <li><a href="#52-自注意力-self-attention" id="markdown-toc-52-自注意力-self-attention">5.2 自注意力 (Self-Attention)</a></li>
    </ul>
  </li>
  <li><a href="#6-tensorflow-官方代码解析" id="markdown-toc-6-tensorflow-官方代码解析">6. tensorflow 官方代码解析</a>    <ul>
      <li><a href="#分段讲解" id="markdown-toc-分段讲解">分段讲解</a></li>
      <li><a href="#参数设置" id="markdown-toc-参数设置">参数设置</a></li>
      <li><a href="#ptbmodel" id="markdown-toc-ptbmodel">PTBModel</a>        <ul>
          <li><a href="#11-lstm结构" id="markdown-toc-11-lstm结构">1.1 LSTM结构</a></li>
          <li><a href="#12-dropoutwrapper" id="markdown-toc-12-dropoutwrapper">1.2 DropoutWrapper</a></li>
          <li><a href="#13-多层lstm结构和状态初始化" id="markdown-toc-13-多层lstm结构和状态初始化">1.3 多层LSTM结构和状态初始化</a></li>
          <li><a href="#2-输入预处理" id="markdown-toc-2-输入预处理">2. 输入预处理</a></li>
          <li><a href="#3-lstm循环" id="markdown-toc-3-lstm循环">3. LSTM循环</a></li>
          <li><a href="#4-损失函数计算" id="markdown-toc-4-损失函数计算">4. 损失函数计算</a></li>
          <li><a href="#5-梯度计算" id="markdown-toc-5-梯度计算">5. 梯度计算</a></li>
          <li><a href="#6-梯度修剪" id="markdown-toc-6-梯度修剪">6. 梯度修剪</a></li>
          <li><a href="#7-优化参数" id="markdown-toc-7-优化参数">7. 优化参数</a></li>
        </ul>
      </li>
      <li><a href="#8-main-函数" id="markdown-toc-8-main-函数">8. Main 函数</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="1-循环神经网络-recurrent-neural-network-rnn">1. 循环神经网络 (Recurrent Neural Network, RNN)</h2>

<p>在机器学习中, 数据表示为 \(n\) 维特征向量 \(\mathbf{x}\in\mathbb{R}^n\) 或 \(h\times w\) 维特征矩阵(如图片), 多层感知机 (multilayer perceptron, MLP) 和卷积神经网络 (convolutional neural network, CNN) 可以提取数据中的特征以进行分类回归等任务.
但通常的 MLP 或 CNN 处理的数据通常认为是独立同分布的, 因此当数据之间存在关联关系时, 这类模型则无法很好的编码数据间的依赖关系, 导致模型的表现较差. 一种典型的数据间依赖关系就是<em>时序关系</em>. 比如话说一半时对方可能就知道了你的意思, 一句话中的代词”他”指代的目标需要分析上下文后才能得到. 时序数据如下图所示, 一列向量则表示一个字的编码.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/time-series.png" data-caption="时序数据" />
    
        
        <div class="container">
            <p>图 1. 时序数据</p>
        </div>
    
</div>

<p>循环神经网络的提出就是为了解决数据中这种典型的时序依赖关系. RNN 是内部包含循环的神经网络 (普通 CNN 不包含循环), RNN 的一个循环单元如下图所示<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/RNN-rolled.png" data-caption="RNN 的循环单元" />
    
        
        <div class="container">
            <p>图 2. RNN 的循环单元</p>
        </div>
    
</div>

<p>其中 \(x_t\) 表示输入序列中时刻 \(t\) 时的值, \(h_t\) 为该层在时刻 \(t\) 的输出. 方块 \(A\) 是一个操作符, 把前一时刻的输出 \(h_{t-1}\) 和当前时刻的输入 \(x_t\) 映射为当前时刻的输出. 注意, \(h_t\) 通常扮演两个角色, 既是循环单元在当前时刻的输出, 又是当前时刻循环单元的<em>状态</em>. 公式表示如下:</p>

\[h_t = \sigma(W_{hx}x_t + W_{hh}h_{t-1}).\]

<p>其中 \(W\) 为权重参数, \(\sigma\) 表示激活函数, 常用的是 \(\tanh(\cdot)\) 函数, 可以把输出的值域控制在 \([-1, 1]\) 之间, 避免在循环过程中不收敛. 我们可以沿着时间轴把上面的循环单元展开, 更加直观.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/RNN-unrolled.png" data-caption="RNN 循环单元展开示意图" />
    
        
        <div class="container">
            <p>图 3. RNN 循环单元展开示意图</p>
        </div>
    
</div>

<p>循环神经网络可以由多层循环单元堆叠而成, 前一个循环单元的输出作为下一循环单元的输入, 如下图所示. 
<!-- 在训练过程中, ... --></p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/multilayer-RNN.png" data-caption="多层循环单元堆叠" />
    
        
        <div class="container">
            <p>图 4. 多层循环单元堆叠</p>
        </div>
    
</div>

<p>RNN 的输入通常表示成<strong>嵌入 (embedding)</strong>的形式, 即构造一个<strong>查询表 (lookup table)</strong>, 把输入序列的每个时刻的特征向量通过查询表转为一个等长的向量. 从而一个序列的形状变为 <code class="language-plaintext highlighter-rouge">[num_time_steps, embedding_size]</code>.</p>

<h3 id="11-rnn-的应用">1.1 RNN 的应用</h3>

<p>RNN 可以根据输入序列的长度和输出序列的长度分为三大类.</p>

<ul>
  <li>多对一: 常用于情感分析, 文本分类</li>
  <li>一对多: Image Caption</li>
  <li>多对多: 机器翻译</li>
  <li>一对一: 退化为 MLP</li>
</ul>

<h3 id="12-rnn-的局限性">1.2 RNN 的局限性</h3>

<p>RNN 也存在一些缺陷:</p>

<ul>
  <li>RNN 可以很好的学习序列中邻近时间步数据点(短期)之间的关系, 但对于长期依赖会变得不稳定.</li>
  <li>RNN 可以把固定长度的输入序列映射到指定长度的输出序列, 但不能动态地根据输入决定输出多长的序列.</li>
</ul>

<p>而 LSTM 和 Encoder-Decoder 的提出解决了这两个问题.</p>

<h2 id="2-长短期记忆-long-short-term-memory-lstm">2. 长短期记忆 (Long Short Term Memory, LSTM)<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></h2>

<p>前面提到, RNN 对于长期依赖经实验表明是不稳定的. 对于短序列, 如一个句子: “The clouds are in the ()”, 括号中预测一个词, 那么很容易根据该词前面的 clouds 和 in 推断出填 sky. 但是对于长序列, 如 “I grew up in France … I speak fluent ()”, 句子中的省略号包含了大量其他信息, 此时最后括号中的词应当根据开头的 France 推断为 French, 但中间大量的无用语句会稀释前期的信息, 导致 RNN 无法正确预测最后的词. 而 Hochreiter &amp; Schmidhuber 提出的 LSTM <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">2</a></sup> 正是解决该问题的.</p>

<p>LSTM 和通常的 CNN 一样为一个循环单元的结构, 但是与 RNN 仅有一个 tanh 激活层不同, LSTM 中包含了更复杂的四层网络的结构设计, 并且四层网络相互耦合, 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM.png" data-caption="LSTM 循环单元展开示意图" />
    
        
        <div class="container">
            <p>图 5. LSTM 循环单元展开示意图</p>
        </div>
    
</div>

<p>上图中的圆角矩形框, 操作符, 分支箭头的含义如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM2-notation.png" data-caption="图例" />
    
        
        <div class="container">
            <p>图 6. 图例</p>
        </div>
    
</div>

<p>下面详细介绍 LSTM 单元的内部结构.</p>

<h3 id="21-lstm-的核心思想">2.1 LSTM 的核心思想</h3>

<p>LSTM 相比于 RNN, 关键在于引入了单元状态(state) \(C\) —— 横穿下图顶部的直线.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-C-line.png" data-caption="单元状态" />
    
        
        <div class="container">
            <p>图 7. 单元状态</p>
        </div>
    
</div>

<p>LSTM 可以通过<strong>门(gate)</strong>来控制向单元状态中增加信息或减少信息. 门由一个 \(sigmoid\) 函数和一个乘法运算符组成, 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-gate.png" data-caption="门" />
    
        
        <div class="container">
            <p>图 8. 门</p>
        </div>
    
</div>

<p>\(sigmoid\) 层输出的值在 \([0, 1]\) 之间, 控制了信息的通过量. 越接近 0, 则表明不允许信息通过(从而形成<em>遗忘</em>); 越接近 1, 则表明允许信息全部通过(从而形成<em>记忆</em>).</p>

<h3 id="22-lstm-单元解析">2.2 LSTM 单元解析</h3>

<p>LSTM 单元在每个时间步需要注意三个向量:</p>
<ul>
  <li>输入的特征向量 \(x_t\)</li>
  <li>上一步输出的特征向量 \(h_{t-1}\)</li>
  <li>上一步结束后的单元状态 \(C_{t-1}\)</li>
</ul>

<p>要注意三个向量是相同的长度.</p>

<p><strong>遗忘门(forget gate).</strong> 每循环一步时, 首先根据上一步的输出 \(h_{t-1}\) 和当前步的输入 \(x_t\) 来决定要遗忘掉上一步的什么信息(从单元状态 \(C_{t-1}\) 中遗忘). 因此只需要计算一个遗忘系数 \(f_t\) 乘到单元状态上即可. 如下图所示, 公式中的方括号表示 \(concat\) 操作.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-focus-f.png" data-caption="遗忘门" />
    
        
        <div class="container">
            <p>图 9. 遗忘门</p>
        </div>
    
</div>

<p><strong>输入门(input gate).</strong> 这一步来决定当前新的输入 \(x_t\) 我们应该把多少信息储存在单元状态中. 这部分有两步, 首先一个输入门计算要保留哪些信息, 得到过滤系数 \(i_t\), 然后使用一个全连接层来从上一步的输出 \(h_{t-1}\) 和当前步的输入 \(x_t\) 中提取特征 \(\tilde{C}_t\). 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-focus-i.png" data-caption="输入门" />
    
        
        <div class="container">
            <p>图 10. 输入门</p>
        </div>
    
</div>

<p><strong>新旧信息合并.</strong> 计算好了遗忘系数, 输入系数, 以及新的要记忆的特征, 现在就可以在单元状态 \(C_{t-1}\) 上执行遗忘操作 \(f_t\ast C_{t-1}\) 和记忆操作 \(+i_t\ast\tilde{C}_t\). 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-focus-C.png" data-caption="新旧信息合并" />
    
        
        <div class="container">
            <p>图 11. 新旧信息合并</p>
        </div>
    
</div>

<p><strong>输出门(output gate).</strong> 最后我们要决定输出什么信息了. 这需要从当前的单元状态 \(C_t\) 来获取要输出的信息. 但显然我们并不会在这一个时间步输出所有记忆的信息, 而是只要输出当前需要的信息, 因此我们用一个输出门来过滤输出的信息, 过滤系数为 \(o_t\). 此外我们希望输出的特征的取值能够介于 \([-1, 1]\) 之间, 因此使用一个 \(tanh\) 函数把单元状态 \(C_t\) 映射到相应的范围, 最后乘上过滤系数得到当前步的输出. 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-focus-o.png" data-caption="输出门" />
    
        
        <div class="container">
            <p>图 12. 输出门</p>
        </div>
    
</div>

<h3 id="23-lstm-单元变体">2.3 LSTM 单元变体</h3>

<p><strong>变体一.</strong> <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">3</a></sup>让所有的门控单元在输出门控系数的时候都可以”看到”当前的单元状态. 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-var-peepholes.png" data-caption="让门控单元可以看到单元状态" />
    
        
        <div class="container">
            <p>图 13. 让门控单元可以看到单元状态</p>
        </div>
    
</div>

<p><strong>变体二.</strong> 让遗忘门的遗忘系数 \(f_t\) 和输入门的输入系数 \(i_t\) 耦合, 即令 \(i_t = 1-f_t\), 从而同时做出哪些信息遗忘以及哪些信息记忆的决策. 这个变体可以让新的有用的记忆”覆盖”老的无用的记忆. 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-var-tied.png" data-caption="遗忘系数和记忆系数耦合" />
    
        
        <div class="container">
            <p>图 14. 遗忘系数和记忆系数耦合</p>
        </div>
    
</div>

<p><strong>变体三(GRU).</strong> <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">4</a></sup>第三种变体更为有名, 称为<strong>门控循环单元(Gated Recurrent Unit, GRU)</strong>. 下一节介绍.</p>

<p><strong>其他参考:</strong> 其他可以参考如下文献:</p>

<ul>
  <li><a href="http://arxiv.org/pdf/1508.03790v2.pdf">Depth Gated RNNs</a></li>
  <li><a href="http://arxiv.org/pdf/1503.04069.pdf">Variants comparision</a></li>
  <li><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Ten thousand RNN architecture tests</a></li>
</ul>

<h2 id="3-门控循环单元gated-recurrent-unit-gru">3. 门控循环单元(Gated Recurrent Unit, GRU)</h2>

<p>GRU<sup id="fnref:8:1" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">4</a></sup> 是一种比 LSTM 稍简单一些的循环单元. GRU 把 LSTM 中的隐藏状态 \(h\) 和单元状态 \(C\) 合并为单个的隐藏状态. 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/LSTM3-var-GRU.png" data-caption="门控循环单元(GRU)" />
    
        
        <div class="container">
            <p>图 15. 门控循环单元(GRU)</p>
        </div>
    
</div>

<p><strong>更新门(update gate).</strong> 更新门系数 \(z_t\) 控制了 \(h_{t-1}\) 中保存的信息(如长期记忆)在当前步保留多少. \(z_t\) 接近 0 时上一步的隐藏状态中的信息 \(h_{t-1}\) 得以保留, 新输入的信息 \(\tilde{h}_t\) 会被忽略; \(z_t\) 接近 1 时则丢弃已有的信息, 并填入新输入的信息.</p>

<p><strong>输入信息的加工.</strong> 当前步输入的信息需要加工后才能合并到隐藏状态 \(h_t\) 中. 输入信息加工时需要参考上一步的隐藏状态 \(h_{t-1}\) 来决定哪些信息有用, 哪些没用. 加工后的信息用 \(\tilde{h}_t\) 表示.</p>

<p><strong>重置门(reset gate).</strong> 重置门系数 \(r_t\) 控制了在加工输入信息的时候使用上一步的隐藏状态中的哪些信息. \(r_t\) 接近 0 时新输入的信息占主导地位, 说明当前步的输入包含的信息与前面的信息关联性很小; \(r_t\) 接近 1 时新输入的信息和前面的长期信息有较大关联性, 需要综合考虑来产生当前步加工后的信息.</p>

<h2 id="4-编码-解码器-encoder-decoder">4. 编码-解码器 (Encoder-Decoder)</h2>

<p>编码-解码器模型是为了实现 \(n\rightarrow m\) 序列映射的模型框架. 这类模型也称为 Sequence to Sequence(seq2seq). 编码器只负责处理输入序列, 并形成输入序列的特征向量后送入解码器. 为了清楚, 我们用公式来表示这个过程<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. 对于输入序列 \(X=\{x_1, x_2, \cdots, x_S\}\) 和期望的输出序列 \(Y=\{y_1, y_2, \cdots, y_T\}\), 我们使用 RNN 模型对其进行建模, 形成一个条件概率 \(P(Y\vert X)\), 使用链式法则解耦如下:</p>

\[P(Y|X) = \prod_{t=1}^TP(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X).\]

<p>那么该 RNN 模型就是一个编码器, 在时刻 \(s\) 的状态通过下式计算:</p>

\[h_s = f_{enc}(h_{s-1}, x_s).\]

<p>编码器的示意图如下图<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">6</a></sup>所示:</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/encoder.png" data-caption="编码器. 输出的中间状态(语义向量) $$ c $$ 可以通过不同的计算公式构造, 形成不同的模型结构." />
    
        
        <div class="container">
            <p>图 16. 编码器. 输出的中间状态(语义向量) $$ c $$ 可以通过不同的计算公式构造, 形成不同的模型结构.</p>
        </div>
    
</div>

<p>解码器 RNN 每个时间步使用前一步的输出 \(y_{t-1}\) 和当前状态 \(g_t\) 产生一个输出 \(y_t\in Y\):</p>

\[\begin{align}
g_1 &amp;= h_s \\
g_t &amp;= f_{dec}(g_{t-1}, y_{t-1})
\end{align}\]

<p>每个时间步的输出概率通过一个线性层和一个 softmax 函数得到:</p>

\[P(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X) = Softmax(Linear(g_t)).\]

<p>使用解码器对语义向量解码. 如果把语义向量只输入解码器的第一个循环时间步, 则形成了 Cho et al.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">7</a></sup> 提出的结构.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/decoder-1.png" data-caption="解码器-1" />
    
        
        <div class="container">
            <p>图 17. 解码器-1</p>
        </div>
    
</div>

<p>如果把语义向量在每一个时间步都输入解码器, 则形成了 Sutskever et al.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">8</a></sup> 提出的结构.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/decoder-2.png" data-caption="解码器-2" />
    
        
        <div class="container">
            <p>图 18. 解码器-2</p>
        </div>
    
</div>

<h3 id="41-编码-解码器模型的局限性">4.1 编码-解码器模型的局限性</h3>

<ul>
  <li>信息的丢失: 整个时间序列只能压缩为一个固定长度的语义向量</li>
  <li>不合理性: seq2seq 的任务中输入序列 \(\{x_0, x_1, \dots, x_{t−1},  x_𝑡, x_{t+1},\dots \}\) 中的每个元素对所有 \(y_s\) 的贡献度是相同的</li>
</ul>

<p>例如: The animal didn’t cross the street because <strong>it</strong> was too tired. 在这句话中, 人是通过综合整句话的信息来判断单词 it 指代的是 the animal, 从而翻译时 the animal 应该对 it 的影响更大.</p>

<p>人们提出了注意力模型来解决普通编码-解码器模型的问题.</p>

<h2 id="5-注意力机制-attention-mechanism">5. 注意力机制 (Attention Mechanism)<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup></h2>

<h3 id="51-什么是注意力">5.1 什么是注意力?</h3>

<p>心理学中对注意力的解释是:</p>

<blockquote>
  <p><strong>Attention</strong> is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information.</p>
</blockquote>

<p>注意力是把有限的资源集中在更重要的目标上. 注意力机制的两个要素:</p>

<ul>
  <li>决定输入信息的哪部分是重要的</li>
  <li>把资源集中分配到重要的信息上</li>
</ul>

<p>沿用编码-解码器模型的结构, 注意力机制通过引入一组归一化的系数 \(\{\alpha_1, \alpha_2, \dots, \alpha_n \}\) 来对输入的信息进行选择, 来解决编码-解码器的不合理性. 这里输入的信息就是指输入序列在 RNN 中的单元输出 \(\mathbf{h}_s\). 归一化的系数 \(\alpha_s\) 用来决定输入信息的重要性, 是编码器输出时对单元输出加权求和系数. 注意力机制在计算不同时间步的输出时, 实时构造编码器输出的语义向量 \(\mathbf{c}_t\), 从而解决了普通编码-解码器信息丢失的问题. 注意力机制如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/attention-1.png" data-caption="注意力机制" />
    
        
        <div class="container">
            <p>图 19. 注意力机制</p>
        </div>
    
</div>

<p>下面讨论加权系数是如何计算的. 考虑到加权系数要反映<strong>输入信息</strong>在当前<strong>时间步</strong>上的重要性, 因此需要把输入信息和时间信息结合起来计算加权系数. 因此通常使用一个 MLP 接收输入信息 \(\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_n\) 和解码器上一时刻的状态 \(\mathbf{s}_{t-1}\) 作为输入, 输出归一化的加权系数 \(\alpha_{t1}, \alpha_{t2}, \dots, \alpha_{tn}\). 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/attention-2.png" data-caption="注意力机制" />
    
        
        <div class="container">
            <p>图 20. 注意力机制</p>
        </div>
    
</div>

<p>注意: 编码-解码器的结构只是注意力机制的一个载体, 注意力机制的核心在于加权系数, 因此可以用于非 RNN 的结构.</p>

<h3 id="52-自注意力-self-attention">5.2 自注意力 (Self-Attention)</h3>

<p>自注意力是一个神经网络模块, 它仍然是使用了注意力机制, 与编码-解码器结构不同的是, 自注意力模块只使用输入的信息计算加权系数, 而不需要上一个时间步的信息, 因此可以实现更大规模的并行计算. Google 在 2017 年提出的 Transformer 实现了可并行的自注意力模块<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">10</a></sup>. 其结构如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2018/01/self-attention.png" data-caption="语言翻译任务中的自注意力" />
    
        
        <div class="container">
            <p>图 21. 语言翻译任务中的自注意力</p>
        </div>
    
</div>

<p>自注意力模块工作方式如下<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">11</a></sup>:</p>

<ul>
  <li>输入的序列首先转化为相同长度的 embedding</li>
  <li>创建三个矩阵: 查询矩阵 (Query), 键矩阵 (Key), 值矩阵 (Value)</li>
  <li>使用这三个矩阵把每个单词的 embedding 映射为三个稍短的特征向量, 分别代表了当前单词的查询向量, 键向量和值向量.</li>
  <li>比如我们要计算单词 Thinking 关于句子中其他单词的注意力时, 使用 Thinking 的查询向量依次与句子中所有单词 (包括 Thinking) 的键向量做点积来计算相似度, 并对相似度进行归一化得到加权系数 (这是 attention 的核心部分).</li>
  <li>加权系数乘到所有单词的值向量上来得到单词 Thinking 经过自注意力模块后输出的特征向量, 这个特征向量中可以看作包含了翻译任务重对准确翻译 Thinking 所需要的信息, 而不包含其他信息 (其他信息在翻译其他词时可能有用, 但在翻译 Thinking 时无用).</li>
</ul>

<h2 id="6-tensorflow-官方代码解析">6. tensorflow 官方代码解析</h2>

<p>来源: <a href="http://blog.csdn.net/u014595019/article/details/52759104">Tensorflow 多层 LSTM 代码分析</a></p>

<p>修改为 tensorflow r1.4 版本</p>

<h3 id="分段讲解">分段讲解</h3>

<p>总的来看，这份代码主要由三步分组成。 
第一部分，是PTBModel,也是最核心的部分，负责tf中模型的构建和各种操作(op)的定义。 
第二部分，是run_epoch函数，负责将所有文本内容分批喂给模型（PTBModel）训练。 
第三部分，就是main函数了，负责将第二部分的run_epoch运行多遍，也就是说，文本中的每个内容都会被重复多次的输入到模型中进行训练。随着训练的进行，会适当的进行一些参数的调整。 
下面就按照这几部分来分开讲一下。我在后面提供了完整的代码，所以可以将完整代码和分段讲解对照着看。</p>

<hr />

<h3 id="参数设置">参数设置</h3>

<p>在构建模型和训练之前，我们首先需要设置一些参数。tf中可以使用 <code class="language-plaintext highlighter-rouge">tf.flags</code> 来进行全局的参数设置</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="n">flags</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">flags</span>
<span class="n">logging</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">logging</span>    

<span class="n">flags</span><span class="p">.</span><span class="nc">DEFINE_string</span><span class="p">(</span>    <span class="c1"># 定义变量 model的值为small, 后面的是注释
</span>    <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">small</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">A type of model. Possible options are: small, medium, large.</span><span class="sh">"</span><span class="p">)</span>

<span class="n">flags</span><span class="p">.</span><span class="nc">DEFINE_string</span><span class="p">(</span><span class="sh">"</span><span class="s">data_path</span><span class="sh">"</span><span class="p">,</span>   <span class="c1">#定义下载好的数据的存放位置
</span>                    <span class="sh">'</span><span class="s">/home/multiangle/download/simple-examples/data/</span><span class="sh">'</span><span class="p">,</span> 
                    <span class="sh">"</span><span class="s">data_path</span><span class="sh">"</span><span class="p">)</span>
<span class="n">flags</span><span class="p">.</span><span class="nc">DEFINE_bool</span><span class="p">(</span><span class="sh">"</span><span class="s">use_fp16</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span>    <span class="c1"># 是否使用 float16格式？
</span>                  <span class="sh">"</span><span class="s">Train using 16-bit floats instead of 32bit floats</span><span class="sh">"</span><span class="p">)</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">flags</span><span class="p">.</span><span class="n">FLAGS</span>     <span class="c1"># 可以使用FLAGS.model来调用变量 model的值。
</span>
<span class="k">def</span> <span class="nf">data_type</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">FLAGS</span><span class="p">.</span><span class="n">use_fp16</span> <span class="k">else</span> <span class="n">tf</span><span class="p">.</span><span class="n">float321234567891011121314151617</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>细心的人可能会注意到上面有行代码定义了 model 的值为 small.这个是什么意思呢？其实在后面的完整代码部分可以看到，作者在其中定义了几个参数类，分别有 small,medium,large 和 test 这 4 种参数。如果 model 的值为small，则会调用 SmallConfig，其他同样。在 SmallConfig 中，有如下几个参数：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">init_scale</span> <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># 相关参数的初始值为随机均匀分布，范围是[-init_scale,+init_scale]
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.0</span>     <span class="c1"># 学习速率,在文本循环次数超过max_epoch以后会逐渐降低
</span><span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mi">5</span>       <span class="c1"># 用于控制梯度膨胀，如果梯度向量的L2模超过max_grad_norm，则等比例缩小
</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>          <span class="c1"># lstm层数
</span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">20</span>          <span class="c1"># 单个数据中，序列的长度。
</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">200</span>       <span class="c1"># 隐藏层中单元数目
</span><span class="n">max_epoch</span> <span class="o">=</span> <span class="mi">4</span>           <span class="c1"># epoch&lt;max_epoch时，lr_decay值=1,epoch&gt;max_epoch时,lr_decay逐渐减小
</span><span class="n">max_max_epoch</span> <span class="o">=</span> <span class="mi">13</span>      <span class="c1"># 指的是整个文本循环次数。
</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span>         <span class="c1"># 用于dropout.每批数据输入时神经网络中的每个单元会以1-keep_prob的概率不工作，可以防止过拟合
</span><span class="n">lr_decay</span> <span class="o">=</span> <span class="mf">0.5</span>          <span class="c1"># 学习速率衰减
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>         <span class="c1"># 每批数据的规模，每批有20个。
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>      <span class="c1"># 词典规模，总共10K个词123456789101112
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>其他的几个参数类中，参数类型都是一样的，只是参数的值各有所不同。</p>

<hr />

<h3 id="ptbmodel">PTBModel</h3>

<p>这个可以说是核心部分了。而具体来说，又可以分成几个小部分：</p>

<ul>
  <li>多层LSTM结构的构建</li>
  <li>输入预处理</li>
  <li>LSTM的循环</li>
  <li>损失函数计算</li>
  <li>梯度计算</li>
  <li>修剪</li>
</ul>

<h4 id="11-lstm结构">1.1 LSTM结构</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">PTBInput</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">The input data.</span><span class="sh">"""</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">batch_size</span>
    <span class="n">self</span><span class="p">.</span><span class="n">num_steps</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_steps</span>
    <span class="n">self</span><span class="p">.</span><span class="n">epoch_size</span> <span class="o">=</span> <span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_steps</span>
    <span class="n">self</span><span class="p">.</span><span class="n">input_data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">reader</span><span class="p">.</span><span class="nf">ptb_producer</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>		<span class="c1"># 该类主要作用是这句, 从文件中读取数据
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>self.input_data 和 self.targets 都是 index 的序列, 尺寸为 [batch_size, num_steps]</strong>. 注意此时不论是input还是target都是用词典id来表示单词的。</p>

<p><code class="language-plaintext highlighter-rouge">PTBModel.__init__()</code> 函数:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">self</span><span class="p">.</span><span class="n">_input</span> <span class="o">=</span> <span class="n">input_</span>	<span class="c1"># [batch_size, num_steps]
</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_</span><span class="p">.</span><span class="n">batch_size</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="n">input_</span><span class="p">.</span><span class="n">num_steps</span>
<span class="n">size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span>		<span class="c1"># 隐藏层规模
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span>	<span class="c1"># 词典规模
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>引进参数.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">lstm_cell</span><span class="p">():</span>
      <span class="k">if</span> <span class="sh">'</span><span class="s">reuse</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">inspect</span><span class="p">.</span><span class="nf">getargspec</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">BasicLSTMCell</span><span class="p">.</span><span class="n">__init__</span><span class="p">).</span><span class="n">args</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> 
                                            <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                                            <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                            <span class="n">reuse</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">get_variable_scope</span><span class="p">().</span><span class="n">reuse</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> 
                                            <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                                            <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>首先使用 <code class="language-plaintext highlighter-rouge">tf.contrib.rnn.BasicLSTMCell</code> 定义单个基本的 LSTM 单元。这里的 size 其实就是隐藏层规模。 
从源码中可以看到，在 LSTM 单元中，有 2 个状态值，分别是 c 和 h，分别对应于下图中的 c 和 h。其中 h 在作为当前时间段的输出的同时，也是下一时间段的输入的一部分。</p>

<div class="polaroid-small">
    
    
    
    <img data-toggle="lightbox" src="/images//2018-1-24/lstm-1.png" data-caption="LSTM 单元" />
    
        
        <div class="container">
            <p>图 22. LSTM 单元</p>
        </div>
    
</div>

<p>那么当 <code class="language-plaintext highlighter-rouge">state_is_tuple=True</code> 的时候，state 是元组形式，state=(c,h)。如果是 False，那么 state 是一个由c和h拼接起来的张量，<code class="language-plaintext highlighter-rouge">state=tf.concat(1, [c,h])</code>。<strong>在运行时，则返回2值，一个是h，还有一个state。</strong></p>

<h4 id="12-dropoutwrapper">1.2 DropoutWrapper</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">attn_cell</span> <span class="o">=</span> <span class="n">lstm_cell</span>
<span class="k">if</span> <span class="n">is_training</span> <span class="ow">and</span> <span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>	<span class="c1"># 在外面包裹 dropout
</span>	<span class="k">def</span> <span class="nf">attn_cell</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nc">DropoutWrapper</span><span class="p">(</span><span class="nf">lstm_cell</span><span class="p">(),</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>我们在这里使用了 dropout 方法。<strong>所谓 dropout, 就是指网络中每个单元在每次有数据流入时以一定的概率(keep prob)正常工作，否则输出0值</strong>。这是是一种有效的正则化方法，可以有效防止过拟合。<em>在 rnn 中使用 dropout 的方法和 cnn 不同</em>，推荐大家去把 <a href="http://arxiv.org/pdf/1409.2329.pdf">recurrent neural network regularization</a> 看一遍。 
在 rnn 中进行 dropout 时，对于 rnn 的部分不进行 dropout，也就是说从 t-1 时候的状态传递到t时刻进行计算时，这个中间不进行 memory 的 dropout；仅在同一个t时刻中，多层 cell 之间传递信息的时候进行 dropout，如下图所示</p>

<div class="polaroid-small">
    
    
    
    <img data-toggle="lightbox" src="/images//2018-1-24/dropout.jpg" data-caption="循环神经元展开示意图" />
    
        
        <div class="container">
            <p>图 23. 循环神经元展开示意图</p>
        </div>
    
</div>

<p>上图中，\(t-2\) 时刻的输入 \(x_{t−2}\) 首先传入第一层 cell，这个过程有 dropout，但是从  \(t−2\) 时刻的第一层 cell 传到 \(t−1, t, t+1\) 的第一层 cell 这个中间都不进行 dropout。再从 \(t+1\) 时候的第一层 cell 向同一时刻内后续的 cell 传递时，这之间又有 dropout 了。</p>

<p>在使用 <code class="language-plaintext highlighter-rouge">tf.contrib.rnn.DropoutWrapper</code> 时，同样有一些参数，例如 <code class="language-plaintext highlighter-rouge">input_keep_prob, output_keep_prob</code> 等，分别控制输入和输出的dropout概率，很好理解。</p>

<h4 id="13-多层lstm结构和状态初始化">1.3 多层LSTM结构和状态初始化</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nc">MultiRNNCell</span><span class="p">([</span><span class="nf">attn_cell</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)],</span>
                                   <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 参数初始化,rnn_cell.RNNCell.zero_stat
</span><span class="n">self</span><span class="p">.</span><span class="n">_initial_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="nf">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="nf">data_type</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>在这个示例中，我们使用了 2 层的 LSTM 网络。也就是说，前一层的 LSTM 的输出作为后一层的输入。使用<code class="language-plaintext highlighter-rouge">tf.contrib.rnn.MultiRNNCell</code> 可以实现这个功能。这个基本没什么好说的，<code class="language-plaintext highlighter-rouge">state_is_tuple</code> 用法也跟之前的类似。构造完多层 LSTM 以后，使用 <code class="language-plaintext highlighter-rouge">zero_state</code> 即可对各种状态进行初始化。</p>

<h4 id="2-输入预处理">2. 输入预处理</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">/cpu:0</span><span class="sh">"</span><span class="p">):</span>
  	<span class="c1"># 把描述单词的指标 idx ([1, 1]) 变为 embedding ([1, hidden_size]) 描述
</span>    <span class="c1"># 使用 embedding 描述可以让网络从描述中学习单词之间的关联, 否则单个的指标之间是独立的, 无法学习关联 ???
</span>    <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">data_type</span><span class="p">())</span>     

    <span class="c1"># 将输入的每个 sequence ([batch_size, num_steps]) 用 embedding 表示 
</span>    <span class="c1"># shape = [batch_size, num_steps, hidden_size]
</span>    <span class="c1"># 所以每个 x_t 都是一个 batch_size x 1 x hidden_size 的向量
</span>    <span class="c1"># 在程序里 1 自动省去, 所以每个 x_t 实际上是 [batch_size, hidden_size] 的向量
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">input_</span><span class="p">.</span><span class="n">input_data</span><span class="p">)</span>

<span class="k">if</span> <span class="n">is_training</span> <span class="ow">and</span> <span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">keep_prob</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>之前有提到过，输入模型的 input 和 target 都是用词典 id 表示的。例如一个句子，“我/是/学生”，这三个词在词典中的序号分别是 0,5,3，那么上面的句子就是 [0,5,3]。这样就和隐藏层需要的输入维度不匹配 (输入需要长度为 hidden_size 的向量)，我们要把词典 id 转化成向量,也就是 embedding 形式。可能有些人已经听到过这种描述了。实现的方法很简单。</p>

<p>第一步，构建一个矩阵，就叫 embedding 好了，尺寸为 [vocab_size, embedding_size]，分别表示词典中单词数目，以及要转化成的向量的维度。一般来说，向量维度越高，能够表现的信息也就越丰富。注意这里的 embedding 变量是使用均匀分布随机初始化的, 初始化器定义在 main 函数中. 并且 embedding 是变量矩阵, 在训练过程中是进行优化的, 期望是让有关联的词语对应的 embedding 向量形成某种关系.</p>

<p>第二步，使用 <code class="language-plaintext highlighter-rouge">tf.nn.embedding_lookup(embedding,input_ids)</code> 假设 input_ids 的长度为 len，那么返回的张量尺寸就为 [len,embedding_size]。</p>

<p>举个栗子:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="c1"># 示例代码
</span><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">InteractiveSession</span><span class="p">()</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">))</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="n">input_embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">initialize_all_variables</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">embedding</span><span class="p">))</span>
<span class="c1">#[[1 0 0 0 0]
# [0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [0 0 0 0 1]]
</span><span class="nf">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_ids</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]}))</span>
<span class="c1">#[[0 1 0 0 0]
# [0 0 1 0 0]
# [0 0 0 1 0]
# [1 0 0 0 0]
# [0 0 0 1 0]
# [0 0 1 0 0]
# [0 1 0 0 0]]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>第三步，如果 keep_prob&lt;1， 那么还需要对输入进行 dropout。不过这边跟 rnn 的 dropout 又有所不同，这边使用 <code class="language-plaintext highlighter-rouge">tf.nn.dropout</code>。</p>

<h4 id="3-lstm循环">3. LSTM循环</h4>

<p>现在，多层 lstm 单元已经定义完毕，输入也已经经过预处理了。那么现在要做的就是将数据输入lstm进行训练了。其实很简单，只要按照文本顺序依次向cell输入数据就好了。lstm上一时间段的状态会自动参与到当前时间段的输出和状态的计算当中。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_initial_state</span> <span class="c1"># state 表示 各个batch中的状态
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="sh">"</span><span class="s">RNN</span><span class="sh">"</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">time_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable_scope</span><span class="p">().</span><span class="nf">reuse_variables</span><span class="p">()</span>
        <span class="c1"># 输入: [batch_size, hidden_size]
</span>        <span class="c1"># 按照顺序向cell输入文本数据, cell_out: [batch_size, hidden_size]
</span>        <span class="p">(</span><span class="n">cell_output</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">=</span> <span class="nf">cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="n">time_step</span><span class="p">,</span> <span class="p">:],</span> <span class="n">state</span><span class="p">)</span> 
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cell_output</span><span class="p">)</span>  <span class="c1"># output: [num_steps][batch_size, hidden_size]
</span>
<span class="c1"># 把之前的list展开，成 [batch, num_steps, hidden_size]
# 然后 reshape 成 [batch*numsteps, hidden_size], 这是为了后面 softmax 层计算方便
</span><span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">outputs</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>这边要注意，<code class="language-plaintext highlighter-rouge">tf.get_variable_scope().reuse_variables()</code> 这行代码不可少，因为在 num_steps 的循环中实际上是在相同的权重上更新的</p>

<h4 id="4-损失函数计算">4. 损失函数计算</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="c1"># softmax_w , shape=[hidden_size, vocab_size], 用于将distributed表示的单词转化为one-hot表示
</span><span class="n">softmax_w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">"</span><span class="s">softmax_w</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">data_type</span><span class="p">())</span>
<span class="n">softmax_b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">"</span><span class="s">softmax_b</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">data_type</span><span class="p">())</span>
<span class="c1"># [batch*numsteps, vocab_size] 从隐藏语义转化成完全表示
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">softmax_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">softmax_b</span>

<span class="c1"># loss , shape=[batch*num_steps], 带权重的交叉熵计算
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">seq2seq</span><span class="p">.</span><span class="nf">sequence_loss</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">,</span>					<span class="c1"># [batch*numsteps, vocab_size]
</span>    <span class="n">input_</span><span class="p">.</span><span class="n">targets</span><span class="p">,</span>			<span class="c1"># [batch_size, num_steps]
</span>    <span class="n">tf</span><span class="p">.</span><span class="nf">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">data_type</span><span class="p">()),</span>	<span class="c1"># weight
</span>    <span class="n">average_across_timesteps</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">average_across_batch</span><span class="o">=</span><span class="bp">True</span>			<span class="c1"># loss = loss / batch_size
</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">_cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">_final_state</span> <span class="o">=</span> <span class="n">state</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>上面代码的上半部分主要用来将多层lstm单元的输出转化成one-hot表示的向量。关于one-hot presentation和distributed presentation的区别，可以参考 <a href="http://blog.csdn.net/u014595019/article/details/51884529#t0">这里</a></p>

<p>代码的下半部分，正式开始计算损失函数。这里使用了 tf 提供的现成的交叉熵计算函数, <code class="language-plaintext highlighter-rouge">tf.contrib.seq2seq.sequence_loss</code>。不知道交叉熵是什么？见<a href="http://blog.csdn.net/u014595019/article/details/52562159#t7">这里</a>. 各个变量的具体shape我都在注释中标明了。注意其中的 <code class="language-plaintext highlighter-rouge">self._targets</code> 是词典 id 表示的。这个函数的具体实现方式不明。我曾经想自己手写一个交叉熵，不过好像 tf 不支持对张量中单个元素的操作。</p>

<h4 id="5-梯度计算">5. 梯度计算</h4>

<p>之前已经计算得到了每批数据的平均误差。那么下一步，就是根据误差来进行参数修正了。当然，首先必须要求梯度</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">self</span><span class="p">.</span><span class="n">_lr</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># lr 指的是 learning_rate
</span><span class="n">tvars</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">trainable_variables</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>通过 tf.trainable_variables 可以得到整个模型中所有 trainable = True 的 Variable</strong>。实际得到的 tvars 是一个列表，里面存有所有可以进行训练的变量。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">clip_by_global_norm</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">tvars</span><span class="p">),</span>
                                  <span class="n">config</span><span class="p">.</span><span class="n">max_grad_norm</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>这一行代码其实使用了两个函数，<code class="language-plaintext highlighter-rouge">tf.gradients</code> 和 <code class="language-plaintext highlighter-rouge">tf.clip_by_global_norm</code>。 我们一个一个来。</p>

<p><strong>tf.gradients</strong> 
用来计算导数。该函数的定义如下所示</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span>
              <span class="n">xs</span><span class="p">,</span>
              <span class="n">grad_ys</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">gradients</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
              <span class="n">gate_gradients</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
              <span class="n">aggregation_method</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>虽然可选参数很多，但是最常使用的还是ys和xs。根据说明得知，ys和xs都可以是一个tensor或者tensor列表。而计算完成以后，该函数会返回一个长为len(xs)的tensor列表，列表中的每个tensor是ys中每个值对xs[i]求导之和。如果用数学公式表示的话，那么 <code class="language-plaintext highlighter-rouge">g = tf.gradients(y,x)</code>可以表示成</p>

\[g_i = \sum_{j=0}^{len(y)}\frac{\partial y_i}{\partial x_i} \\ g = [g_0, g_1, \cdots, g_{len(x)}\]

<h4 id="6-梯度修剪">6. 梯度修剪</h4>

<p><strong>tf.clip_by_global_norm</strong> 
修正梯度值，用于<strong>控制梯度爆炸的问题</strong>。梯度爆炸和梯度弥散的原因一样，都是因为链式法则求导的关系，导致梯度的指数级衰减。为了避免梯度爆炸，需要对梯度进行修剪。 
先来看这个函数的定义：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">clip_by_global_norm</span><span class="p">(</span><span class="n">t_list</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">,</span> <span class="n">use_norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>输入参数</strong>中：t_list 为待修剪的张量, clip_norm 表示修剪比例 (clipping ratio).</p>

<p>函数<strong>返回2个参数</strong>： list_clipped，修剪后的张量，以及global_norm，一个中间计算量。当然如果你之前已经计算出了 global_norm 值，你可以在 use_norm 选项直接指定 global_norm 的值。</p>

<p>那么具体<strong>如何计算</strong>呢？根据源码中的说明，可以得到 
<code class="language-plaintext highlighter-rouge">list_clipped[i] = t_list[i] * clip_norm / max(global_norm, clip_norm)</code>,其中 
<code class="language-plaintext highlighter-rouge">global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</code></p>

<p>如果你更熟悉数学公式，则可以写作</p>

<p>\(L_c^i = \frac{L_t^i N_c}{\max(N_c, N_g)},\quad N_g = \sqrt{\sum_i(L_t^i)^2}\)
其中, \(L_t^i\) 和 \(L_c^i\) 代表 <code class="language-plaintext highlighter-rouge">t_list[i]</code> 和 <code class="language-plaintext highlighter-rouge">list_clipped[i]</code>, \(N_c\) 和 \(N_g\) 代表 clip_norm 和 global_norm 的值。</p>

<h4 id="7-优化参数">7. 优化参数</h4>

<p>之前的代码已经求得了合适的梯度，现在需要使用这些梯度来更新参数的值了。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># 梯度下降优化，指定学习速率
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="nc">GradientDescentOptimizer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_lr</span><span class="p">)</span>
<span class="c1"># optimizer = tf.train.AdamOptimizer()
# optimizer = tf.train.GradientDescentOptimizer(0.5)
</span><span class="n">self</span><span class="p">.</span><span class="n">_train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span>
    <span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">tvars</span><span class="p">),</span>
    <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">framework</span><span class="p">.</span><span class="nf">get_or_create_global_step</span><span class="p">())</span>  <span class="c1"># 将梯度应用于变量
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>这一部分就比较自由了，tf 提供了很多种优化器，例如最常用的梯度下降优化（GradientDescentOptimizer）也可以使用 AdamOptimizer。这里使用的是梯度优化。值得注意的是，这里使用了optimizer.apply_gradients来将求得的梯度用于参数修正，而不是之前简单的optimizer.minimize(cost)</p>

<p>还有一点，要留心一下 self._train_op，只有该操作被模型执行，才能对参数进行优化。如果没有执行该操作，则参数就不会被优化。</p>

<h3 id="8-main-函数">8. Main 函数</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">random_uniform_initializer</span><span class="p">(</span><span class="o">-</span><span class="n">config</span><span class="p">.</span><span class="n">init_scale</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">init_scale</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">name_scope</span><span class="p">(</span><span class="sh">"</span><span class="s">Train</span><span class="sh">"</span><span class="p">):</span>
	<span class="n">train_input</span> <span class="o">=</span> <span class="nc">PTBInput</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">TrainInput</span><span class="sh">"</span><span class="p">)</span>
	<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variable_scope</span><span class="p">(</span><span class="sh">"</span><span class="s">Model</span><span class="sh">"</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">):</span>
    	<span class="n">m</span> <span class="o">=</span> <span class="nc">PTBModel</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">input_</span><span class="o">=</span><span class="n">train_input</span><span class="p">)</span>
	<span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="nf">scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">cost</span><span class="p">)</span>
	<span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="nf">scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">Learning Rate</span><span class="sh">"</span><span class="p">,</span> <span class="n">m</span><span class="p">.</span><span class="n">lr</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>以均匀分布 [-init_scale, init_scale] 作为所有变量的初始化器, <code class="language-plaintext highlighter-rouge">train_input</code> 是一个类, 包含了所有的训练数据.</p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p><strong>Understanding LSTM Networks – Colah’s blogs</strong> <br />
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">[link]</a> OnLine. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:6" role="doc-endnote">

      <p><strong>Long short-term memory</strong> <br />
Hochreiter S, Schmidhuber J. <br />
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&amp;rep=rep1&amp;type=pdf">[link]</a> In Neural computation, 1997, 9(8): 1735-1780. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">

      <p><strong>Recurrent nets that time and count</strong> <br />
Gers F A, Schmidhuber J. <br />
<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">[link]</a> In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. 2000. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">

      <p><strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong> <br />
Cho K, Van Merriënboer B, Gulcehre C, et al. <br />
<a href="https://arxiv.org/pdf/1406.1078">[link]</a> In arXiv preprint arXiv:1406.1078, 2014. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5" role="doc-endnote">

      <p><strong>Order Matters: Sequence to Sequence for Sets</strong> <br />
Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. <br />
<a href="http://arxiv.org/abs/1511.06391.">[link]</a> In ArXiv:1511.06391, November. 2015. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><strong>详解从 Seq2Seq模型、RNN结构、Encoder-Decoder模型 到 Attention模型</strong> <br />
<a href="https://caicai.science/2018/10/06/attention%E6%80%BB%E8%A7%88/">[link]</a> OnLine. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p><strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong> <br />
Cho K, Van Merriënboer B, Gulcehre C, et al. <br />
<a href="https://arxiv.org/abs/1406.1078">[link]</a> In arXiv preprint arXiv:1406.1078, 2014. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">

      <p><strong>Sequence to Sequence Learning with Neural Networks</strong> <br />
Ilya Sutskever, Oriol Vinyals, Quoc V.Le.  <br />
<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks">[link]</a> In Advances in neural information processing systems. 2014: 3104-3112. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">

      <p><strong>Neural machine translation by jointly learning to align and translate</strong> <br />
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio <br />
<a href="https://arxiv.org/abs/1409.0473">[link]</a> arXiv preprint arXiv:1409.0473, 2014. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">

      <p><strong>Attention is all you need</strong> <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin <br />
<a href="https://arxiv.org/abs/1706.03762">[link]</a> Advances in neural information processing systems. 2017: 5998-6008. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">

      <p><strong>The Illustrated Transformer</strong>  <br />
Jay Alammar <br />
<a href="https://jalammar.github.io/illustrated-transformer/">[link]</a> OnLine. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        </article>
        <hr>

        <!-- 
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
         -->

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2017/12/14/MaskX-RCNN/">MaskX R-CNN 阅读笔记</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2018/04/01/Faster-RCNN-Code-Analysis/">Faster R-CNN(Tensorflow) 源码解析</a></p>
        
    </div>
</div>


        <!-- <h2 id="comments">Comments</h2>
        



 -->


    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right-jekyll">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id]):not([class])')
    for (var i = 0; i < aTags.length; i++) {
        if (aTags[i].getAttribute('href').startsWith('#'))
        {
            continue
        }
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>

    <footer class="site-footer">


    <div class="wrapper">

        <p class="contact">
            联系我: 
            <a href="https://github.com/jarvis73" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:zjw.math@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>   
            <a href="https://www.zhihu.com/people/lin-xi-1-1" title="Zhihu"><i class="iconfont icon-daoruzhihu"></i></a>      
        </p>
        <p>
            <i class="fa fa-eye" style="padding-right: 2px;"></i> 访问量: <span id="busuanzi_value_site_pv"></span>次 | <i class="fa fa-user" style="padding-right: 2px;"></i> 访客数<span id="busuanzi_value_site_uv"></span>人次
        </p>
        <p class="power">
            网站支持 <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a> | 主题支持 <a href="https://github.com/Gaohaoyang/gaohaoyang.github.io">HyG</a> & <a href="https://github.com/Jarvis73/jarvis73.github.io">Jarvis73</a>
        </p>
        <p>
            <img src="/images/misc/beian.png" style="padding-right: 2px; padding-bottom: 4px; vertical-align: middle;" /> <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo" class="beian" >浙公网安备 33010602011353号 | </a>
            <a target="_blank" href="https://beian.miit.gov.cn/" class="beian">浙ICP备2020038513号-1</a>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script type='text/javascript' src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>
    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/lib/jquery/jquery.js" charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/js/zui.min.js" charset="utf-8"></script>
    <script src="/js/index_page.js" charset="utf-8"></script>
    <script src="/js/functions.js" charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
  </body>

</html>
