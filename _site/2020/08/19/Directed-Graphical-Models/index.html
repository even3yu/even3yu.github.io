<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>机器学习(三): 有向图模型 (Directed Graphical Models)</title>
    <meta name="description" content="">

    <!-- 网站所有权验证 -->
    <meta name="baidu-site-verification" content="code-kzX4R1yDEi" />
    <meta name="google-site-verification" content="kj0sMKl0iZFsV2KPqmN9OJ3S7aeCrJnNYAOTpJzXCz4" />
    <meta name="msvalidate.01" content="C9A829578EE81A43ECA102B601A5E052" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/css/zui.min.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2020/08/19/Directed-Graphical-Models/">
    <link rel="alternate" type="application/rss+xml" title="Jarvis' Blog (总有美丽的风景让人流连)" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?b9d127980a49e998bbedb8aab536a81d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108096001-2', 'auto');
      ga('send', 'pageview');

    </script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [["$$ "," $$"],["\\(","\\)"]],
    processEscapes: true,
    tags: "all",
    macros: {
      bm: ["{\\boldsymbol #1}",1]
    },
    packages: {'[+]': ['noerrors']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['input/asciimath', '[tex]/noerrors']
  }
};
</script>
<script async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" id="MathJax-script">
</script>
<!-- src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> -->



<!--  -->
    
<script type="text/javascript">
    var host = "jarvis73.com";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
      window.location.protocol = "https";
</script>
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/index.html" class="brand">Jarvis' Blog (总有美丽的风景让人流连)</a>
        <small>总有美丽的风景让人流连</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/index.html">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/wiki/">
                        
                            <i class="fa fa-book"></i>Wiki
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left-jekyll">
        <h1>机器学习(三): 有向图模型 (Directed Graphical Models)</h1>
        <div class="label-custom">

            <div class="label-custom-card">
                <i class="fa fa-calendar"></i>2020-08-19
            </div>

            

            <div class="label-custom-card">
                <i class="fa fa-user"></i>Jarvis
                
            </div>

            <div class="label-custom-card">
                <i class="fa fa-key"></i>Post  
            </div>

            <div class="label-custom-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#机器学习" title="Category: 机器学习" rel="category">机器学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#1-介绍" id="markdown-toc-1-介绍">1. 介绍</a>    <ul>
      <li><a href="#11-链式法则" id="markdown-toc-11-链式法则">1.1 链式法则</a></li>
      <li><a href="#12-条件独立" id="markdown-toc-12-条件独立">1.2 条件独立</a></li>
      <li><a href="#13-图模型" id="markdown-toc-13-图模型">1.3 图模型</a></li>
      <li><a href="#14-有向图模型" id="markdown-toc-14-有向图模型">1.4 有向图模型</a></li>
    </ul>
  </li>
  <li><a href="#2-例子" id="markdown-toc-2-例子">2. 例子</a>    <ul>
      <li><a href="#21--朴素贝叶斯分类器" id="markdown-toc-21--朴素贝叶斯分类器">2.1  朴素贝叶斯分类器</a></li>
      <li><a href="#22-马尔可夫和隐马尔可夫模型" id="markdown-toc-22-马尔可夫和隐马尔可夫模型">2.2 马尔可夫和隐马尔可夫模型</a></li>
      <li><a href="#23-医疗诊断" id="markdown-toc-23-医疗诊断">2.3 医疗诊断</a></li>
    </ul>
  </li>
  <li><a href="#3-推断" id="markdown-toc-3-推断">3. 推断</a></li>
  <li><a href="#4-学习" id="markdown-toc-4-学习">4. 学习</a></li>
  <li><a href="#5-dgms-中的条件独立性" id="markdown-toc-5-dgms-中的条件独立性">5. DGMs 中的条件独立性</a>    <ul>
      <li><a href="#51-d-划分和贝叶斯球算法-全局马尔可夫性" id="markdown-toc-51-d-划分和贝叶斯球算法-全局马尔可夫性">5.1 d-划分和贝叶斯球算法 (全局马尔可夫性)</a>        <ul>
          <li><a href="#511-链" id="markdown-toc-511-链">5.1.1 链</a></li>
          <li><a href="#512-分叉" id="markdown-toc-512-分叉">5.1.2 分叉</a></li>
          <li><a href="#513-v-型结构" id="markdown-toc-513-v-型结构">5.1.3 V-型结构</a></li>
          <li><a href="#514-边界条件" id="markdown-toc-514-边界条件">5.1.4 边界条件</a></li>
        </ul>
      </li>
      <li><a href="#52-dgms-的其他马尔可夫性质" id="markdown-toc-52-dgms-的其他马尔可夫性质">5.2 DGMs 的其他马尔可夫性质</a>        <ul>
          <li><a href="#521-有向局部马尔可夫性-l" id="markdown-toc-521-有向局部马尔可夫性-l">5.2.1 有向局部马尔可夫性 L</a></li>
          <li><a href="#522-有序马尔可夫性-o" id="markdown-toc-522-有序马尔可夫性-o">5.2.2 有序马尔可夫性 O</a></li>
          <li><a href="#523-有向全局马尔可夫性-g" id="markdown-toc-523-有向全局马尔可夫性-g">5.2.3 有向全局马尔可夫性 G</a></li>
        </ul>
      </li>
      <li><a href="#53-马尔可夫覆盖和全条件" id="markdown-toc-53-马尔可夫覆盖和全条件">5.3 马尔可夫覆盖和全条件</a></li>
    </ul>
  </li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<blockquote>
  <p>处理复杂系统的两个原理:</p>
  <ol>
    <li>模块化原理</li>
    <li>抽象化原理</li>
  </ol>

  <p>– Michael Jordan</p>
</blockquote>

<p>\(\newcommand{\x}{\mathbf{x}} \newcommand{\tb}{\boldsymbol{\theta}}\) 
假设我们观测到了多个互相关联的变量, 如文档中的单词, 图像中的像素, 或者基因. 那么我们如何紧凑的表示多个关联变量的联合分布 \(p(\x\vert\tb)\) 呢? 我们如何使用这个分布在给定一些变量之后推理另一些变量呢?</p>

<p>本文内容来自 Machine Learning: A Probabilistic Perspective 的第 10 章 Directed Graphical Models (Bayes Nets)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
原书错误修订<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.
其他资料<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<h2 id="1-介绍">1. 介绍</h2>

<h3 id="11-链式法则">1.1 链式法则</h3>

<blockquote>
  <p>根据概率的链式法则, 我们有</p>

\[p(\x_{1:V})=p(x_1)p(x_2\vert x_1)p(x_3\vert x_2,x_1)p(x_4\vert x_1,x_2,x_3)\dots p(x_V\vert x_{1:V-1})\]

  <p>其中 \(V\) 是变量的个数, 为了简便, 我们省略掉了条件变量 \(\tb\) .</p>
</blockquote>

<p>假设所有变量都有 \(K\) 个状态, 那么 \(p(x_1)\) 可以表示为含有 \(O(K)\) 个值的表 (即离散分布, 实际上根据和为 1 的约束有 \(K-1\) 个自由变量). 类似地, 条件分布 \(p(x_2\vert x_1)\) 可以表示为含有 \(O(K^2)\) 个值的二维表, 其中 \(p(x_2=j\vert x_1=i)=T_{ij}\) , 我们说 \(\mathbf{T}\) 是一个<strong>随机矩阵 (stochastic matrix)</strong>, 因为它的每一行都满足 \(\sum_jT_{ij}=1\), 且每一个元素的值都满足 \(0\leq T_{ij}\leq 1\) . 类似地, \(p(x_3\vert x_1,x_2)\) 是一个有 \(O(K^3)\) 个元素的三维表. 这些表都称为<strong>条件概率表 (conditional probability tables, CPTs)</strong>, 它的元素 (即模型参数) 数量随着变量的个数呈指数增长, 这对于建模来说是很差的一个选择.</p>

<h3 id="12-条件独立">1.2 条件独立</h3>

<p>缩减参数规模的一个方法就是增加模型假设, 其中<strong>条件独立 (conditional independence, CI)</strong> 的假设最为常用.</p>

<blockquote>
  <p>随机变量 \(X\) 和 \(Y\) 关于 \(Z\) 条件独立, 记为  \(X\perp Y\vert Z\) ,  当且仅当条件联合分布可以写为条件边际分布的乘积:</p>

\[X\perp Y\vert Z \Longleftrightarrow p(X,Y\vert Z)=p(X\vert Z)p(Y\vert Z)\]

</blockquote>

<p>那么如果我们如果假设 \(x_{t+1}\perp x_{1:t-1}\vert x_t\) , 即 “给定现在, 未来与过去式独立的”. 该性质称为(一阶)<strong>马尔可夫假设 (Markov assumption)</strong> . 基于该假设和链式法则, 我们可以把联合分布写为</p>

\[p(\x_{1:V})=p(x_1)\prod_{t=1}^Vp(x_t\vert x_{t-1})\]

<p>该模型称为(一阶)<strong>马尔可夫链 (Markov chain)</strong> . 它可以通过一个初始分布 \(p(x_1=i)\) 和一个状态转移矩阵 \(p(x_t=j\vert x_{t-1}=i)\) 来表示.</p>

<h3 id="13-图模型">1.3 图模型</h3>

<p>尽管一阶马尔可夫链可以表示一维随机序列的分布 (即随机过程), 但二维图像, 三维视频, 和其他高维度的数据如何表示呢? 因此这里引入<strong>图模型 (graphical model, GM)</strong> . 图模型是条件独立假设下, 联合分布的一种表示方式. 图中的节点表示随机变量, 边表示变量间的条件独立假设. 图模型有多种类别, 如有向图, 无向图, 有向和无向的结合图. 下面我们对图模型的概念做一个列举用于参考.</p>

<p>图 \(G=(\mathcal{V}, \mathcal{E})\) 包含一族<strong>节点 (nodes/vertices)</strong> \(\mathcal{V}=\{1,\dots,V\}\) 和一族<strong>边 (edges)</strong> \(\mathcal{E}=\{(s,t):s,t\in\mathcal{V}\}\) . 我们可以使用<strong>邻接矩阵 (adjacency matrix)</strong> 表示图, 其中 \(G(s,t)=1\) 用来表示 \((s,t)\in\mathcal{E}\) . 如果 \(G(s,t)=1\Leftrightarrow G(t,s)=1\) , 那么我们说图是<strong>无向的 (undirected)</strong> , 否则图是 <strong>有向的 (directed)</strong> , 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-0.png" data-caption="有向图和无向图" />
    
        
        <div class="container">
            <p>图 1. 有向图和无向图</p>
        </div>
    
</div>

<p>我们通常假设 \(G(s,s)=0\) , 即没有自循环. 以下列举一下其他术语:</p>

<table>
  <thead>
    <tr>
      <th>术语</th>
      <th>解释</th>
      <th>备注</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>父节点 Parent</strong></td>
      <td>对于有向图, 节点的父节点是所有指向它的节点</td>
      <td>\(pa(s)\triangleq \{t:G(t,s)=1\}\)</td>
    </tr>
    <tr>
      <td><strong>子节点 Child</strong></td>
      <td>对于有向图, 节点的子节点是所有它指向的节点</td>
      <td>\(ch(s)\triangleq \{t:G(s,t)=1\}\)</td>
    </tr>
    <tr>
      <td><strong>家族节点 Family</strong></td>
      <td>对于有向图, 节点的家族节点是它和它的父节点</td>
      <td>\(fam(s)=\{s\}\cup pa(s)\)</td>
    </tr>
    <tr>
      <td><strong>根节点 Root</strong></td>
      <td>对于有向图, 根节点是没有父节点的节点</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>叶节点 Leaf</strong></td>
      <td>对于有向图, 叶节点是没有子节点的节点</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>祖先节点 Ancestors</strong></td>
      <td>对于有向图, 节点的祖先节点是父节点, 祖父节点等长辈节点</td>
      <td>\(anc(t)\triangleq\{s:s\leadsto t\}\)</td>
    </tr>
    <tr>
      <td><strong>后代节点 Descendants</strong></td>
      <td>对于有向图, 节点的后代节点是子节点, 孙子节点等晚辈节点</td>
      <td>\(desc(t)\triangleq\{t:s\leadsto t\}\)</td>
    </tr>
    <tr>
      <td><strong>邻居节点 Neighbors</strong></td>
      <td>对于图, 节点的邻居节点是所有与其相连的节点</td>
      <td>\(nbr(s)\triangleq\{t:G(s,t)=1\vee G(t,s)=1\}\)</td>
    </tr>
    <tr>
      <td><strong>度 Degree</strong></td>
      <td>对于图, 节点的度是邻居节点的个数. 有向图包含出度和入度</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>圈 Cycle/Loop</strong></td>
      <td>对于图, 圈是可以首位相连的一族节点. 有向图的圈称为有向圈</td>
      <td>\(s_1-s_2-s_3-s_1,\;1\rightarrow2\rightarrow3\rightarrow1\)</td>
    </tr>
    <tr>
      <td><strong>有向无环图 DAG</strong></td>
      <td>没有圈的有向图</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>拓扑顺序 Topological ordering</strong></td>
      <td>对于DAG, 拓扑顺序是节点的一种顺序编号, 使得父节点的编号小于子节点</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>路径 Path/Trail</strong></td>
      <td>从一个节点到另一个节点的一族有向边</td>
      <td>\(s\leadsto t\)</td>
    </tr>
    <tr>
      <td><strong>树 Tree</strong></td>
      <td>无向树是无向无环图, 有向树是DAG</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>多项树 Polytree</strong></td>
      <td>允许树的节点有多个父节点</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>子图 Subgraph</strong></td>
      <td>\(A\) 的子图是其中的部分节点和关联的边构成的图</td>
      <td>\(G_A=(\mathcal{V}_A,\mathcal{E}_A)\)</td>
    </tr>
    <tr>
      <td><strong>团 Clique</strong></td>
      <td>对于无向图, 团是两两互相连接的一族节点. <strong>最大团</strong>是不能再拓展节点的团</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="14-有向图模型">1.4 有向图模型</h3>

<p><strong>有向图模型 (directed graphical model, DGM)</strong> 是一个基于 DAG 的图模型, 也成为<strong>贝叶斯网络 (Bayesian networks)</strong> 或 <strong>信念网络 (belief networks)</strong> . 但 DGM 和”贝叶斯”和”信念”都没什么太大关系. DAG 的主要特性就是拓扑顺序. 给定拓扑顺序, 我们给出<strong>有序马尔可夫性 (ordered Markov property)</strong> 的假设, 即节点只依赖与它的父节点, 而不依赖于父节点以外的祖先节点:</p>

\[x_s\perp\x_{pred(s)\backslash pa(s)}\vert \x_{pa(s)}\]

<p>这是一阶马尔可夫性质从链到 DAG 的自然推广. 以前面的有向图为例, 其联合分布可以表示为:</p>

\[\newcommand{\cancel}{\enclose{updiagonalstrike}}
\begin{align}\require{enclose}
p(\x_{1:5}) &amp;= p(x_1)p(x_2\vert x_1)p(x_3\vert x_1,\cancel{x_2})p(x_4\vert \cancel{x_1},x_2,x_3)p(x_5\vert \cancel{x_1}\cancel{x_2},x_3,\cancel{x_4}) \\
&amp;= p(x_1)p(x_2\vert x_1)p(x_3\vert x_1)p(x_4\vert x_2,x_3)p(x_5\vert x_3)
\end{align}\]

<p>一般地, 我们有</p>

\[p(\x_{1:V}\vert G)=\prod_{t=1}^Vp(x_t\vert \x_{pa(t)})\]

<p>其中每一项 \(p(x_t\vert \x_{pa(t)})\) 都是一个<strong>条件概率密度 (conditional probability distribution, CPD)</strong> . 我们把分布记为 \(p(\x\vert G)\) 是为了强调上面的等式只有在 DAG 的条件独立假设下才成立.</p>

<h2 id="2-例子">2. 例子</h2>

<p>本节介绍一些常用的概率模型表示为 DGM 的例子.</p>

<h3 id="21--朴素贝叶斯分类器">2.1  朴素贝叶斯分类器</h3>

<p><strong>朴素贝叶斯分类器 (naive Bayes classifier)</strong> 假设在给定类别标签时特征是相互独立的, 如下图 (a) 所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-1.png" data-caption="(a) 朴素贝叶斯分类器表示为 DGM, X 为观测变量, Y 为隐藏变量. (b) 树增广的朴素贝叶斯分类器" />
    
        
        <div class="container">
            <p>图 2. (a) 朴素贝叶斯分类器表示为 DGM, X 为观测变量, Y 为隐藏变量. (b) 树增广的朴素贝叶斯分类器</p>
        </div>
    
</div>

<p>从而联合分布可以写为:</p>

\[p(y,\x)=p(y)\prod_{j=1}^Dp(x_j\vert y)\]

<p>朴素贝叶斯分类器之所以简单, 是因为特征独立性的假设. 但如果我们希望减弱这个假设, 从而可以对特征之间进行建模, 那么可以利用 DGM. 如果模型是树, 那么这个方法就变为<strong>树增广的朴素贝叶斯分类器 (tree-augmented naive Bayes classifier, TAN)</strong> , 如上图 (b) 所示.</p>

<h3 id="22-马尔可夫和隐马尔可夫模型">2.2 马尔可夫和隐马尔可夫模型</h3>

<p>前面我们介绍过一阶马尔可夫模型. 如果当前状态依赖于前面两个节点的状态, 那么我们就得到了二阶马尔可夫模型, 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-2.png" data-caption="(a) 一阶马尔可夫链. (b) 二阶马尔可夫链" />
    
        
        <div class="container">
            <p>图 3. (a) 一阶马尔可夫链. (b) 二阶马尔可夫链</p>
        </div>
    
</div>

<p>但二阶马尔可夫模型仍然难以建模链上变量的长期依赖. 另一种办法就是假设有一个潜在的随机过程, 可以通过一阶马尔可夫链建模, 而数据是该随机过程的观测值, 如下图所示. \(z_t\) 是潜变量, \(x_t\) 是观测变量. \(p(z_t\vert z_{t-1})\) 是变换模型, \(p(\x_t\vert z_t)\) 是观测模型.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-3.png" data-caption="隐马尔可夫模型" />
    
        
        <div class="container">
            <p>图 4. 隐马尔可夫模型</p>
        </div>
    
</div>

<h3 id="23-医疗诊断">2.3 医疗诊断</h3>

<ul>
  <li>ICU 中的<strong>报警网络 (alarm network)</strong></li>
</ul>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-4.png" data-caption="alarm network" />
    
        
        <div class="container">
            <p>图 5. alarm network</p>
        </div>
    
</div>

<ul>
  <li>诊断时的<strong>快速医疗参考 (quick medical reference)</strong>网络</li>
</ul>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-5.png" data-caption="quick medical reference" />
    
        
        <div class="container">
            <p>图 6. quick medical reference</p>
        </div>
    
</div>

<p>详见第 10.2.3 节的内容.</p>

<h2 id="3-推断">3. 推断</h2>

<p>当我们得到一个 DGM 的联合分布 \(p(\x_{1:V}\vert \tb)\) 后, 就可以用来做概率推断. 这里我们假设 \(\tb\) 已知了, 具体下一节再讨论怎么学习一个好的 \(\tb\) .</p>

<p>我们首先把变量分为<strong>可见变量 (visible variables)</strong> \(\x_v\) 和<strong>隐藏变量 (hidden variables)</strong> \(\x_h\) . <strong>推断 (inference)</strong> 指的就是给定可见变量来计算隐藏变量的后验分布:</p>

\[p(\x_h\vert \x_v,\tb)=\frac{p(\x_h,\x_v\vert \tb)}{p(\x_v\vert \tb)}=\frac{p(\x_h,\x_v\vert \tb)}{\sum_{\x_h'}p(\x_h',\x_v\vert \tb)}\]

<p>有时我们仅关心部分隐藏变量, 我们把这部分变量称为<strong>查询变量 (query variables)</strong> \(\x_q\), 而剩下的隐藏变量称为<strong>无关变量 (nuisance variables)</strong> \(\x_n\). 那么通过边缘化无关变量即可得到查询变量的计算方法:</p>

\[p(\x_q\vert \x_v,\tb)=\sum_{\x_n}p(\x_q,\x_n\vert \x_v,\tb)\]

<h2 id="4-学习">4. 学习</h2>

<p>学习指的是学习模型参数, 即给定数据计算模型参数的<strong>最大后验概率 (maximum a posterior, MAP)</strong> 估计:</p>

\[\hat{\tb}=\underset{\tb}{\arg\max}\sum_{i=1}^N\log p(\x_{i,v}\vert \tb)+\log p(\tb)\]

<p>其中 \(\x_{i,v}\) 是第 \(i\) 个样本的可见变量. 如果先验是均匀分布 \(p(\tb)\propto1\) 或者数据量 \(N\) 非常大, 那么上式变成<strong>极大似然估计 (maximum likelyhood estimation, MLE)</strong> .</p>

<p>从完全数据 (complete data) 中学习参数是容易的, 因为上式中的似然是可以因式分解的; 但如果数据不完全, 即包含隐藏变量, 那么似然函数无法分解, 学习也变得更加困难.</p>

<h2 id="5-dgms-中的条件独立性">5. DGMs 中的条件独立性</h2>

<p>图模型的基础是条件独立行假设. 在图 \(G\) 中给定 \(C\) 的条件下 \(A\) 和 \(B\) 独立可以记为 \(\x_A\perp_G\x_B\vert \x_C\) . 令 \(I(G)\) 表示图 \(G\) 中所有的条件独立陈述.</p>

<p>我们说 \(G\) 是对于分布 \(p\) 是一个 <strong>I-map</strong> (independence map) , 或者说 \(p\) 是关于 \(G\) 的 Markov, 当且仅当 \(I(G)\subseteq I(p)\) , 其中 \(I(p)\) 是分布 \(p\) 满足的所有条件独立陈述的集合. 换句话说, 如果图模型没有对分布做任何非真的断言, 那么这个图模型就是个 I-map. 这是为了让图模型作为分布的一个安全的代理模型. 注意, 完全图的图模型一定是 I-map, 因为它包含了所有的边 (即没有对分布的独立性做任何断言).</p>

<p>如果 \(G\) 是 \(p\) 的一个 I-map, 并且不存在 \(G'\subseteq G\) 是 \(p\) 的 I-map, 那么我们说 \(G\) 是 \(p\) 的<strong>最小 I-map</strong> .</p>

<h3 id="51-d-划分和贝叶斯球算法-全局马尔可夫性">5.1 d-划分和贝叶斯球算法 (全局马尔可夫性)</h3>

<p>我们首先介绍一些定义. 我们说无向路径 \(P\) 是由一族边 \(E\) (包含证据的) <strong>d-可分 (d-separated)</strong> 的, 当且仅当满足以下至少一个条件:</p>

<ol>
  <li>\(P\) 包含一条链, \(s\rightarrow m\rightarrow t\) 或 \(s\leftarrow m\leftarrow t\) , 其中 \(m\in E\)</li>
  <li>\(P\) 包含一个分叉, \(s\swarrow^m\searrow t\) , 其中 \(m\in E\)</li>
  <li>\(P\) 包含一个 v 型结构, \(s\searrow_m\swarrow t\) , 其中 \(m\notin E\) , 并且 \(m\) 的任何后代也不属于 \(E\) .</li>
</ol>

<p>因此, 我们说给定一族观测节点 \(E\), 一族节点 \(A\) 与另一族节点 \(B\) 是 d-可分的当且仅当从 \(a\in A\) 到 \(b\in B\) 的每一条无向路径都是在 \(E\) 下 d-可分的. 这样, 我们可以定义 DAG 的条件独立性质如下:</p>

\[\x_A\perp_G\x_B\vert \x_E \Longleftrightarrow A \text{ is d-separated from } B \text{ given } E.\]

<p><strong>贝叶斯球 (Bayes ball algorithm)</strong> 算法是一种简便的判断在 \(E\) 下 \(A\) 是否 d-可分于 \(B\) 的算法. 其要点在于, 我们遮住 \(E\) 中的节点, 即他们被观测到了, 我们然后把”球”放到 \(A\) 中的每一个节点处, 让他们按照一定的规则四处弹, 然后考察是否有球可以到达 \(B\) 中的节点, 规则有三条, 如下图所示:</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-6.png" data-caption="贝叶斯球算法的三条规则 (a-c) 为存在观察变量的规则, (e-f) 为无观察变量时的规则" />
    
        
        <div class="container">
            <p>图 7. 贝叶斯球算法的三条规则 (a-c) 为存在观察变量的规则, (e-f) 为无观察变量时的规则</p>
        </div>
    
</div>

<p>三条规则描述如下 (小球运动时不考虑边的方向):</p>

<ol>
  <li>小球可以通过一个链, 但中间存在观测变量时<strong>无法</strong>通过</li>
  <li>小球可以通过分叉结构, 但中间存在观测变量时<strong>无法</strong>通过</li>
  <li>小球<strong>不</strong>可以通过 V-型结构, 但中间存在观测变量时可以通过</li>
</ol>

<p>判断这三条规则的方法如下:</p>

<h4 id="511-链">5.1.1 链</h4>

<p>考虑链式结构 \(X\rightarrow Y\rightarrow Z\) , 其联合分布如下:</p>

\[p(x,y,z)=p(x)p(y\vert x)p(z\vert y)\]

<p>那么, 在观测到 \(y\)  时, 我们有</p>

\[p(x,z\vert y)=\frac{p(x)p(y\vert x)p(z\vert y)}{p(y)}=\frac{p(x,y)p(z\vert y)}{p(y)}=p(x\vert y)p(z\vert y)\]

<p>因此有 \(x\perp y\vert z\) .</p>

<h4 id="512-分叉">5.1.2 分叉</h4>

<p>考虑分叉结构 \(X\leftarrow Y\rightarrow Z\) , 联合分布为:</p>

\[p(x,y,z)=p(y)p(x\vert y)p(z\vert y)\]

<p>那么, 在观测到 \(y\) 时, 我们有</p>

\[p(x,z\vert y)=\frac{p(y)p(x\vert y)p(z\vert y)}{p(y)}=p(x\vert y)p(z\vert y)\]

<p>因此有 \(x\perp y\vert z\) .</p>

<h4 id="513-v-型结构">5.1.3 V-型结构</h4>

<p>考虑 V-型结构 \(X\rightarrow Y\leftarrow Z\) , 联合分布为:</p>

\[p(x,y,z)=p(x)p(z)p(y\vert x,z)\]

<p>那么, 在观测到 \(y\) 时, 我们有</p>

\[p(x,z\vert y)=\frac{p(x)p(z)p(x,z\vert y)}{p(y)}\]

<p>所以 \(x\not\perp z\vert y\) . 然而, 在没有观测到 \(y\) 时, 我们有</p>

\[p(x,z)=p(x)p(z)\]

<p>即 \(x\) 和 \(z\) 是独立的, 因此我们知道在 V-型结构下, 两个独立的变量在观测到他们的共同后代时就不再独立了. 这种现象称为 <strong>explaining away</strong> 或 <strong>inter-causal reasoning</strong> 或 <strong>Berkson’s paradox</strong>.</p>

<p>针对于 explaining away 的这种现象, 我们举个例子来理解. 假设我投掷了两枚硬币, 其结果用 0 和 1 表示, 那么两枚硬币的投掷结果就是随机变量 \(X\) 和 \(Z\), 两次投掷的结果之和为随机变量 \(Y\) . 那么如果我们不知道两次投掷的结果之和是多少, 那么显然两枚硬币的结果是相互独立的. 但是如果我们知道了两次投掷的和是多少, 那么两枚硬币各自的结果就耦合起来了. 比如我们预先知道和为 1, 那么第一枚硬币的观测结果就直接决定了第二枚硬币的观测结果 (即不再独立).</p>

<h4 id="514-边界条件">5.1.4 边界条件</h4>

<p>贝叶斯球也需要边界条件, 下图 (a), (b).</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-7.png" data-caption="边界条件" />
    
        
        <div class="container">
            <p>图 8. 边界条件</p>
        </div>
    
</div>

<p>为了理解边界条件的含义, 如上图 (c) 所示. 假设 \(Y'\) 是 \(Y\) 的一个无噪声的副本, 那么这就意味着如果我们观测到了 \(Y'\) , 那么我们就知道了 \(Y\) , 这样两个父节点 \(X\) 和 \(Y\) 就需要耦合起来解释该观测变量. 所以, 如果我们让小球沿着 \(X\rightarrow Y\rightarrow Y'\) 的路径滚动, 它就应当沿着 \(Y'\rightarrow Y\rightarrow Z\) 的路径反弹回来. 然而, 如果 \(Y\) 和 \(Y'\) 都是隐藏变量 (即无法观测的), 那么小球就不会反弹.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/DGM-8.png" data-caption="DGM 的例子 (对原书的图做了修改)" />
    
        
        <div class="container">
            <p>图 9. DGM 的例子 (对原书的图做了修改)</p>
        </div>
    
</div>

<p>比如上图.</p>

<p>例1: 有 \(x_2\perp x_6\vert x_5,x_1\) , 因为路径均被上述三个准则阻断:</p>

<ul>
  <li>\(2\rightarrow5\rightarrow6\) 被 \(x_5\) 阻断 ( \(x_5\) 被观测到了)</li>
  <li>\(2\rightarrow4\rightarrow7\rightarrow6\) 被 \(x_7\) 阻断 ( \(x_7\) 没有被观测到)</li>
  <li>\(2\rightarrow1\rightarrow3\rightarrow6\) 被 \(x_1\) 阻断 ( \(x_1\) 被观测到了)</li>
</ul>

<p><strong>注:</strong> 这个例子在原书<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>中是完全错误的<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, 因此这里为了给出一个正确的例子, 修改了图 (去掉了 \(3\rightarrow5\) ), 并增加了 \(x_1\) 作为观测到的变量.</p>

<p>例2:  \(x_2\not\perp x_6\vert x_5,x_7\) , 因为 \(2\rightarrow4\rightarrow7\rightarrow6\) 没有被 \(x_7\) 阻断 ( \(x_7\) 被观测到了).</p>

<h3 id="52-dgms-的其他马尔可夫性质">5.2 DGMs 的其他马尔可夫性质</h3>

<h4 id="521-有向局部马尔可夫性-l">5.2.1 有向局部马尔可夫性 L</h4>

<p>从 d-划分的准则, 我们可以总结出<strong>有向局部马尔可夫性 (directed local Markov property)</strong>:</p>

\[t\perp nd(t)\backslash pa(t)\vert pa(t)\]

<p>其中非后代 (non-descendants)  \(nd(t)=\mathcal{V}\backslash\{t\cup desc(t)\}\) . 比如上图中, 有 \(3\perp2,4\vert 1\) .</p>

<h4 id="522-有序马尔可夫性-o">5.2.2 有序马尔可夫性 O</h4>

<p><strong>有序马尔科夫性 (ordered Markov property)</strong>:</p>

\[t\perp pred(t)\backslash pa(t)\vert pa(t)\]

<p>因为 \(pred(t)\subseteq nd(t)\) . 比如上图中, 有 \(5\perp 1\vert 2,3\) .</p>

<h4 id="523-有向全局马尔可夫性-g">5.2.3 有向全局马尔可夫性 G</h4>

\[\x_A\perp_G\x_B\vert \x_E \Longleftrightarrow A \text{ is d-separated from } B \text{ given } E.\]

<p>显然, 我们有 \(G\Longrightarrow L\Longrightarrow O\) , 另外可以证明有 \(O\Longrightarrow L\Longrightarrow G\) , 因此这三个马尔可夫性质是等价的.</p>

<h3 id="53-马尔可夫覆盖和全条件">5.3 马尔可夫覆盖和全条件</h3>

<p>使得节点 \(t\) 条件独立于图中其他所有节点的一族节点称为<strong>马尔科夫覆盖 (Markov blanket)</strong> , 记为 \(mb(t)\) . 可以证明, 马尔科夫覆盖满足:</p>

\[mb(t)\triangleq ch(t)\cup pa(t)\cup copa(t)\]

<p>其中 \(copa(t)\) 表示<strong>协父节点 (co-parent)</strong> , 即子节点的其他父节点. 如上面图 9 中的例子, 我们有</p>

\[mb(5)=\{6,7\}\cup\{2\}\cup\{4\}=\{2,4,6,7\}\]

<p>节点 \(t\) 的全条件表示为:</p>

\[p(x_t\vert\x_{-t})\propto p(x_t\vert\x_{pa(t)})\prod_{s\in ch(t)}p(x_s\vert\x_{pa(s)})\]

<p>全条件在 Gibbs 采样中非常重要.</p>

<h2 id="参考文献">参考文献</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p><strong>Machine Learning: A Probabilistic Perspective</strong><br />
Kevin P. Murphy <br />
<a href="https://mitpress.mit.edu/books/machine-learning-1">[html]</a>, <a href="http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf">[pdf]</a>, <a href="https://www.cs.ubc.ca/~murphyk/MLbook/index.html">[index]</a>, Chapter 10, Directed graphical models (Bayes nets) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><strong>MAJOR bug fixes/changes to appear in printing #4 (out September 2013)</strong><br />
<a href="https://docs.google.com/document/d/157A0Po-v7_D2cudjI8nMhr_cK5pl03YYtZM4xXbFyOQ/edit">[html]</a>
<a href="https://www.cs.ubc.ca/~murphyk/MLbook/errata.html">[index]</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p><strong>Directed Graphical Models</strong><br />
John Lafferty, Han Liu, and Larry Wasserman<br />
<a href="http://www.stat.cmu.edu/~larry/=sml/DAGs.pdf">[pdf]</a>, Chapter 18 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        </article>
        <hr>

        <!-- 
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
         -->

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2020/07/09/Intelligent-Scissors/">智能剪刀 (Intelligent Scissors)</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2020/08/20/Mixture-Models/">机器学习(四): 混合模型和EM算法 (Mixture Models and EM Algorithm)</a></p>
        
    </div>
</div>


        <!-- <h2 id="comments">Comments</h2>
        



 -->


    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right-jekyll">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id]):not([class])')
    for (var i = 0; i < aTags.length; i++) {
        if (aTags[i].getAttribute('href').startsWith('#'))
        {
            continue
        }
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>

    <footer class="site-footer">


    <div class="wrapper">

        <p class="contact">
            联系我: 
            <a href="https://github.com/jarvis73" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:zjw.math@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>   
            <a href="https://www.zhihu.com/people/lin-xi-1-1" title="Zhihu"><i class="iconfont icon-daoruzhihu"></i></a>      
        </p>
        <p>
            <i class="fa fa-eye" style="padding-right: 2px;"></i> 访问量: <span id="busuanzi_value_site_pv"></span>次 | <i class="fa fa-user" style="padding-right: 2px;"></i> 访客数<span id="busuanzi_value_site_uv"></span>人次
        </p>
        <p class="power">
            网站支持 <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a> | 主题支持 <a href="https://github.com/Gaohaoyang/gaohaoyang.github.io">HyG</a> & <a href="https://github.com/Jarvis73/jarvis73.github.io">Jarvis73</a>
        </p>
        <p>
            <img src="/images/misc/beian.png" style="padding-right: 2px; padding-bottom: 4px; vertical-align: middle;" /> <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo" class="beian" >浙公网安备 33010602011353号 | </a>
            <a target="_blank" href="https://beian.miit.gov.cn/" class="beian">浙ICP备2020038513号-1</a>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script type='text/javascript' src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>
    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/lib/jquery/jquery.js" charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/js/zui.min.js" charset="utf-8"></script>
    <script src="/js/index_page.js" charset="utf-8"></script>
    <script src="/js/functions.js" charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
  </body>

</html>
