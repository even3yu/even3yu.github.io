<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>机器学习(四): 混合模型和EM算法 (Mixture Models and EM Algorithm)</title>
    <meta name="description" content="">

    <!-- 网站所有权验证 -->
    <meta name="baidu-site-verification" content="code-kzX4R1yDEi" />
    <meta name="google-site-verification" content="kj0sMKl0iZFsV2KPqmN9OJ3S7aeCrJnNYAOTpJzXCz4" />
    <meta name="msvalidate.01" content="C9A829578EE81A43ECA102B601A5E052" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/css/zui.min.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2020/08/20/Mixture-Models/">
    <link rel="alternate" type="application/rss+xml" title="Jarvis' Blog (总有美丽的风景让人流连)" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?b9d127980a49e998bbedb8aab536a81d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-108096001-2', 'auto');
      ga('send', 'pageview');

    </script>



<script>
window.MathJax = {
  tex: {
    inlineMath: [["$$ "," $$"],["\\(","\\)"]],
    processEscapes: true,
    tags: "all",
    macros: {
      bm: ["{\\boldsymbol #1}",1]
    },
    packages: {'[+]': ['noerrors']}
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
  loader: {
    load: ['input/asciimath', '[tex]/noerrors']
  }
};
</script>
<script async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js" id="MathJax-script">
</script>
<!-- src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> -->



<!--  -->
    
<script type="text/javascript">
    var host = "jarvis73.com";
    if ((host == window.location.host) && (window.location.protocol != "https:"))
      window.location.protocol = "https";
</script>
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/index.html" class="brand">Jarvis' Blog (总有美丽的风景让人流连)</a>
        <small>总有美丽的风景让人流连</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/index.html">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/wiki/">
                        
                            <i class="fa fa-book"></i>Wiki
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>Collections
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left-jekyll">
        <h1>机器学习(四): 混合模型和EM算法 (Mixture Models and EM Algorithm)</h1>
        <div class="label-custom">

            <div class="label-custom-card">
                <i class="fa fa-calendar"></i>2020-08-20
            </div>

            

            <div class="label-custom-card">
                <i class="fa fa-user"></i>Jarvis
                
            </div>

            <div class="label-custom-card">
                <i class="fa fa-key"></i>Post  
            </div>

            <div class="label-custom-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#机器学习" title="Category: 机器学习" rel="category">机器学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#1-潜变量模型" id="markdown-toc-1-潜变量模型">1. 潜变量模型</a></li>
  <li><a href="#2-混合模型" id="markdown-toc-2-混合模型">2. 混合模型</a>    <ul>
      <li><a href="#21-高斯混合模型" id="markdown-toc-21-高斯混合模型">2.1 高斯混合模型</a></li>
      <li><a href="#22--multinoullis-混合模型" id="markdown-toc-22--multinoullis-混合模型">2.2  Multinoullis 混合模型</a></li>
      <li><a href="#23-使用混合模型聚类" id="markdown-toc-23-使用混合模型聚类">2.3 使用混合模型聚类</a></li>
    </ul>
  </li>
  <li><a href="#3-em-算法" id="markdown-toc-3-em-算法">3. EM 算法</a>    <ul>
      <li><a href="#31-基本思想" id="markdown-toc-31-基本思想">3.1 基本思想</a></li>
      <li><a href="#32-gmms-的-em-算法" id="markdown-toc-32-gmms-的-em-算法">3.2 GMMs 的 EM 算法</a>        <ul>
          <li><a href="#321-辅助函数" id="markdown-toc-321-辅助函数">3.2.1 辅助函数</a></li>
          <li><a href="#322-e-step" id="markdown-toc-322-e-step">3.2.2 E-step</a></li>
          <li><a href="#323-m-step" id="markdown-toc-323-m-step">3.2.3 M-step</a></li>
          <li><a href="#324-k-means-算法" id="markdown-toc-324-k-means-算法">3.2.4 K-means 算法</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<blockquote>
  <p>如何对多个变量的依赖关系进行建模?</p>
  <ol>
    <li>利用图模型: 存在关系的变量之间用边连接.</li>
    <li>利用潜变量: 假设多个变量是从同样的”诱因”产生.</li>
  </ol>
</blockquote>

<p>本文内容来自 Machine Learning: A Probabilistic Perspective 的第 11 章 Mixture models and the EM algorithm<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
原书错误修订<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 id="1-潜变量模型">1. 潜变量模型</h2>

<p>上面第2点的”诱因”就是我们说的<strong>潜变量 (latent variable)</strong>, 潜变量是不能直接观测到的, 包含潜变量的模型称为<strong>潜变量模型 (latent variable models, LVM)</strong>. LVM 相比于图模型有以下优缺点:</p>
<ul>
  <li>优点: 通常来说参数更少</li>
  <li>缺点: 拟合起来更困难 (因为包含不可观测的变量)</li>
</ul>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/MM-0.png" data-caption="潜变量模型和图模型的参数量对比" />
    
        
        <div class="container">
            <p>图 1. 潜变量模型和图模型的参数量对比</p>
        </div>
    
</div>

<p>如上图所示, 假设所有节点都是二值的, 那么左侧的模型有 17 个自由参数, 右侧的模型有 59 个自由参数. 计算参数数量的方式如下: 1) 当前变量为二值变量, 所以根据归一化原则只需要一个自由参数 \(2 - 1=1\); 2) 当前变量存在条件变量时, 相应的自由参数数量随条件变量的数量呈指数增长 \(2^n\). 比如上图左侧中间的 H 节点, 有三个条件变量 (入度), 那么其自由参数的数量为 \((2 - 1)\times 2^3=8\) 个.</p>

<p>利用潜变量, 我么可以得到丰富的 LVM, 如下图所示, 有 (a) 多对多模型, (b) 一对多模型, (c) 多对一模型 和 (d) 一对一模型.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/MM-1.png" data-caption="潜变量模型表示为 DGM" />
    
        
        <div class="container">
            <p>图 2. 潜变量模型表示为 DGM</p>
        </div>
    
</div>

<h2 id="2-混合模型">2. 混合模型</h2>

<p>\(\newcommand{\cat}{\text{Cat}} \newcommand{\x}{\mathbf{x}} \newcommand{\tb}{\boldsymbol{\theta}} \newcommand{\mub}{\boldsymbol{\mu}} \newcommand{\Sigmab}{\boldsymbol{\Sigma}}\) 
LVM 最简单的形式就是当 \(z_i\in\{1,\dots,K\}\) 表示一个离散潜变量, 我们通常使用一个离散先验 \(p(z_i)=\cat(\pi)\) . 对于似然, 我们用条件分布来表示 \(p(\x_i\vert z_i=k)=p_k(\x_i)\) , 其中 \(p_k\) 是第 k 个观测变量的<strong>基分布 (base distribution)</strong> , 可以是任何分布类型. 潜变量和观测变量合并起来就得到了<strong>混合模型 (mixture model)</strong> , 因为我们混合了 K 个基分布:</p>

\[p(\x_i\vert\tb)=\sum_{k=1}^K\pi_kp_k(\x_i\vert\tb)\]

<p>因为我们采用了加权平均的方式, 且混合权重 \(\pi_k\) 满足 \(0\leq\pi_k\leq1\) , 并且 \(\sum_{k=1}^K\pi_k=1\) , 所以这是 \(p_k\) 的凸组合.</p>

<h3 id="21-高斯混合模型">2.1 高斯混合模型</h3>

<p>混合模型中使用最为广泛的就是<strong>高斯混合模型 (mixture of Gaussians, MOG or Gaussian mixture model, GMM)</strong> . 在 GMM 中, 每个基分布都是均值为 \(\mub_k\) 协方差矩阵为 \(\Sigmab_k\) 的多元高斯分布, 因此 GMM 的形式为:</p>

\[p(\x_i\vert\tb)=\sum_{k=1}^K\mathcal{N}(\x_i\vert\mub_k,\Sigmab_k)\]

<p>下图是包含 3 个分量的 GMM 的例子.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/MM-2.png" data-caption="3 个高斯分布的混合模型" />
    
        
        <div class="container">
            <p>图 3. 3 个高斯分布的混合模型</p>
        </div>
    
</div>

<blockquote>
  <p>在高斯分布的数量足够大的情况下, GMM 可以用来近似 \(\mathbb{R}^D\) 中的任意密度分布.</p>
</blockquote>

<h3 id="22--multinoullis-混合模型">2.2  Multinoullis 混合模型</h3>

<p>假设我们的数据是是由 \(D\) 维的比特向量组成, 那么类条件密度为:</p>

\[p(\x_i\vert z_i=k,\tb)=\prod_{j=1}^D\text{Ber}(x_{ij}\vert\mu_{ij})=\prod_{j=1}^D\mu_{jk}^{x_{ij}}(1-\mu_{jk})^{1-x_{ij}}\]

<p>其中 \(\mu_{jk}\) 是第 \(k\) 类中第 \(j\) 个比特为开的状态(反之为闭)的概率.</p>

<p>引入潜变量后, 潜变量并没有实际含义, 只是用来使得模型更强. 可以证明该混合模型的均值和协方差为:</p>

\[\begin{align}
\mathbb{E}[\x] &amp;=\sum_k\pi_k\mub_k \\
\text{cov}[\x] &amp;=\sum_k\pi_k[\Sigmab_k+\mub_k\mub_k^T]-\mathbb{E}[\x]\mathbb{E}[\x]^T
\end{align}\]

<p>其中 \(\Sigmab_k=\text{diag}(\mu_{jk}(1-\mu_{jk}))\) . 所以尽管每个成分的分布是可分解的, 但联合分布是不可分解的. 因此混合分布可以建模变量间的关系, 而简单的 Bernoullis 分布的乘积却不能.</p>

<h3 id="23-使用混合模型聚类">2.3 使用混合模型聚类</h3>

<p>混合模型主要有两种应用:</p>

<ul>
  <li>当作黑盒密度模型 \(p(\x_i)\) , 可以用于数据压缩, 离群值检测, 生成式分类器</li>
  <li>用于聚类</li>
</ul>

<p>聚类的方法是, 首先拟合混合模型, 然后计算后验概率 \(p(z_i=k\vert\x_i,\tb)\) , 它表示数据点 \(\x_i\) 属于第 \(k\) 个类别的概率, 或称为<strong>责任 (responsibility)</strong> , 通过贝叶斯公式计算如下:</p>

\[r_{ik}\triangleq p(z_i=k\vert\x_i,\tb)=\frac{p(z_i=k\vert\tb)p(\x_i\vert z_i=k,\tb)}{\sum_{k'=1}^Kp(z_i=k'\vert\tb)p(\x_i\vert z_i=k',\tb)}\]

<p>这个方法称为<strong>软聚类 (soft clustering)</strong> , 这个计算过程和生成式分类器相同. 软聚类和生成式分类器的区别在于训练过程: 前者无法观测 \(z_i\) 的值, 而生成式分类器观测了 \(y_i\) 的值 (扮演了 \(z_i\) 的角色).</p>

<p>我们可以通过 \(1-\max_k r_{ik}\) 来表示聚类的不确定性程度. 假设该值很小, 那么我们就可以使用<strong>最大后验 (maximum a posterior, MAP)</strong> 计算<strong>硬聚类 (hard clustering)</strong> , 为:</p>

\[z_i^*=\underset{k}{\arg\max}~r_{ik}=\underset{k}{\arg\max}[\log p(\x_i\vert z_i=k,\tb)+\log p(z_i=k\vert\tb)]\]

<p>这里给出二值数据聚类的一个例子, MNIST 手写数字数据集, 我们忽略类别标签, 拟合一个 Multinoullis  的混合模型, 使用 \(K=10\) , 并可视化出聚类中心 \(\hat{\mub_k}\) , 如下图所示.</p>

<div class="polaroid">
    
    
    
    <img data-toggle="lightbox" src="/images/2020/08/MM-3.png" data-caption="MNIST 混合模型聚类中心" />
    
        
        <div class="container">
            <p>图 4. MNIST 混合模型聚类中心</p>
        </div>
    
</div>

<p>注意这 10 个聚类中心和数字的 10 个类别标签并不一致, 这是因为模型潜在的变量不止 10 个, 比如数字 7 就可以有带把和不带把两种写法, 因此如果要区分开 10 个数字类别, 就需要大于 10 个的聚类数. <strong>这告诉我们, 聚类的结果不一定是可靠的, 在实际应用中需要小心斟酌.</strong></p>

<h2 id="3-em-算法">3. EM 算法</h2>

<p>对于大多数机器学习和统计学习模型来说, 如果我们可以观测到所有随机变量的值, 那么在使用 MLE 或 MAP 进行参数估计时是非常容易的. 但是, 如果存在缺失数据或潜变量时, 就很难计算 MLE 或 MAP 了.</p>

<p>一种办法是利用基于梯度的优化器寻找<strong>负对数似然 (negative log likelihook, NLL)</strong> 的局部极小值:</p>

\[\text{NLL}(\tb)\triangleq-\frac1N\log p(\mathcal{D}\vert\tb)\]

<p>然而, 潜变量模型往往存在一些约束, 如协方差矩阵要是正定的, 混合权重的和为 1 等, 在梯度优化时比较麻烦. 因此, 这里引入<strong>期望最大化 (expectation maximization, EM)</strong> 算法. 这是一种简单的迭代算法, 每一步可以求闭式解, 同时该算法自动保证了约束成立.</p>

<p>EM 算法的思路是: 如果数据完整的观测到了, 那么我们就可以采用 MLE/MAP 估计. 因此 EM 算法分为两步:</p>

<ul>
  <li>给定参数推断缺失的数据 (<strong>E-step</strong>)</li>
  <li>给定填充好的数据优化参数 (<strong>M-step</strong>)</li>
</ul>

<h3 id="31-基本思想">3.1 基本思想</h3>

<p>令 \(\x_i\) 是第 i 个观测变量, \(z_i\) 是隐藏变量, 我们的目的是最大化观测数据的对数似然:</p>

\[l(\tb)=\sum_{i=1}^N\log p(\x_i\vert\tb)=\sum_{i=1}^N\log\left[\sum_{z_i}p(\x_i,z_i\vert\tb)\right]\]

<p>但是因为 \(\log\) 不能移到求和符号内部, 因此上式难以优化. EM 算法按如下方式解决问题. 定义<strong>完全数据的对数似然</strong>为:</p>

\[l_c(\tb)\triangleq\sum_{i=1}^N\log p(\x_i,z_i\vert\tb)\]

<p>因为 \(\z_i\) 未知, 所以上式无法计算, 所以我们再定义一个<strong>期望完全数据的对数似然</strong>为:</p>

\[Q(\tb,\tb^{t-1})=\mathbb{E}[l_c(\tb)\vert\mathcal{D},\tb^{t-1}]\]

<p>其中 \(t\) 是当前迭代的步数, \(Q\) 是辅助函数. 期望是对上一步的参数 \(\tb^{t-1}\) 和观测数据 \(\mathcal{D}\) 取的. <strong>E-step</strong> 的目的是计算 \(Q(\tb,\tb^{t-1})\) 或 \(Q\) 中 MLE 依赖的项, 而 <strong>M-step</strong> 是关于 \(\tb\) 优化 \(Q\) :</p>

\[\tb^t=\underset{\tb}{\arg\max}~Q(\tb,\tb^{t-1})\]

<p>如果是计算 MAP 估计, 上式修改为:</p>

\[\tb^t=\underset{\tb}{\arg\max}~Q(\tb,\tb^{t-1}) + \log p(\tb)\]

<h3 id="32-gmms-的-em-算法">3.2 GMMs 的 EM 算法</h3>

<p>本小节介绍如何使用 EM 算法拟合 GMMs. 我们假设高斯混合模型的成分数量 \(K\) 是已知的 (实际中可以作为超参数).</p>

<h4 id="321-辅助函数">3.2.1 辅助函数</h4>

<p>期望完全数据的对数似然为:</p>

\[\begin{align}
Q(\tb,\tb^{t-1})
&amp;\triangleq \mathbb{E}\left[\sum_i\log p(\x_i,z_i\vert\tb)\right] \\
&amp;=          \sum_i\mathbb{E}\left[\log\left[\prod_{k=1}^K(\pi_k p(\x_i\vert\tb_k))^{\mathbb{I}(z_i=k)}\right]\right] \\ 
&amp;=          \sum_i\sum_k\mathbb{E}[\mathbb{I}(z_i=k)]\log[\pi_k p(\x_i\vert\tb_k)] \\
&amp;=          \sum_i\sum_k r_{ik}\log\pi_k + \sum_i\sum_k r_{ik}\log p(\x_i\vert\tb_k)
\end{align}\]

<p>其中 \(r_{ik}\triangleq p(z_i=k\vert\x_i,\tb^{t-1})\) 是 k 个聚类对于第 i 个样本的责任 (参看本文第 2.3 节).</p>

<h4 id="322-e-step">3.2.2 E-step</h4>

<p>E-stem 用来计算 \(r_{ik}\) (因为 MLE 依赖于它, 见 3.1 节):</p>

\[r_{ik}=\frac{\pi_k p(\x_i\vert\tb_k^{t-1})}{\sum_{k'}\pi_{k'} p(\x_i\vert\tb_{k'}^{t-1})}\]

<h4 id="323-m-step">3.2.3 M-step</h4>

<p>在 M-stem 中, 我们关于 \(\boldsymbol{\pi}\) 和 \(\tb_k\) 优化 \(Q\) . 对于 \(\boldsymbol{\pi}\) , 我们有:</p>

\[\pi_k=\frac1N\sum_i r_{ik} =\frac{r_k}{N}\]

<p>其中 \(r_k=\sum_i r_{ik}\) 是属于第 \(k\) 类的点的加权数量. 对于 \(\tb_k\) , 即高斯分布的 \(\mub_k\) 和 \(\Sigmab_k\) , 从 \(Q\) 中最后结果的第二项我们有:</p>

\[\begin{align}
l(\mub_k,\Sigmab_k) 
&amp;= \sum_k\sum_i r_{ik}\log p(\x_i\vert\tb_k) \\
&amp;= -\frac12\sum_i r_{ik}[\log\vert\Sigmab_k\vert + (\x_i-\mub_k)^T\Sigmab_k^{-1}(\x_i-\mub_k)]
\end{align}\]

<p>上式其实就是计算<strong>多元高斯分布 (multivariate Gaussian, MVN)</strong> 的 MLE. 可以证明新的参数估计为:</p>

\[\begin{align}
\mub_k &amp;= \frac{\sum_i r_{ik}\x_i}{r_k} \\
\Sigmab_k &amp;= \frac{\sum_i r_{ik}(\x_i-\mub_k)(\x_i-\mub_k)^T}{r_k}=\frac{\sum_i r_{ik}\x_i\x_i^T}{r_k}-\mub_k\mub_k^T
\end{align}\]

<p>上面的式子是直观的: 聚类 \(k\) 的中心就是所有赋以第 \(k\) 类的样本点加权平均, 协方差矩阵正比于加权的经验<strong>散布矩阵 (scatter matrix)</strong> . 上面两式的求法类似于 MVN 的 MLE 求法, 后者在原书中<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 的第 4 章 4.1.3.1 节给出了详细的证明过程.</p>

<h4 id="324-k-means-算法">3.2.4 K-means 算法</h4>

<p>K-means 聚类算法可以看作 GMMs 的 EM 算法的一个特例. 考虑上述 GMM 增加两个假设:</p>

<ul>
  <li>\(\Sigmab_k=\sigma^2\mathbf{I}_D\) 对于所有类别 \(k\) 是固定的</li>
  <li>\(\pi_k=1/K\) 也是固定的</li>
</ul>

<p>因此只有聚类中心 \(\mub_k\) 是需要估计的. 考虑使用如下的 \(delta\) 函数近似 E-step 中的后验计算:</p>

\[p(z_i=k\vert\x_i,\tb)\approx\mathbb{I}(k=z_i^*)\]

<p>其中 \(z_i^*=\arg\max_k p(z_i=k\vert\x_i,\tb)\) , 这称为<strong>硬 EM (hard EM)</strong>, 因为我们没有采取加权而是直接赋值的形式来指定数据点的类别. 因为我们假设每个聚类的协方差矩阵都是相等的单位阵, 所以 \(\x_i\) 最可能被分到的聚类可以用最近的原型来表示:</p>

\[z_i^*=\underset{k}{\arg\min}\Vert\x_i-\mub_k\Vert_2^2\]

<p>因此在 E-step, 我们在 \(N\) 个数据点中确定 \(K\) 个聚类中心. 在 M-step 中, 通过第 (22) 式计算新的聚类中心:</p>

\[\mub_k=\frac1{N_k}\sum_{i:z_i=k}\x_i\]

<h2 id="参考文献">参考文献</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p><strong>Machine Learning: A Probabilistic Perspective</strong><br />
Kevin P. Murphy <br />
<a href="https://mitpress.mit.edu/books/machine-learning-1">[html]</a>, <a href="http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf">[pdf]</a>, <a href="https://www.cs.ubc.ca/~murphyk/MLbook/index.html">[index]</a>, Chapter 11, Mixture models and the EM algorithm <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><strong>MAJOR bug fixes/changes to appear in printing #4 (out September 2013)</strong><br />
<a href="https://docs.google.com/document/d/157A0Po-v7_D2cudjI8nMhr_cK5pl03YYtZM4xXbFyOQ/edit">[html]</a>
<a href="https://www.cs.ubc.ca/~murphyk/MLbook/errata.html">[index]</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        </article>
        <hr>

        <!-- 
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
         -->

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2020/08/19/Directed-Graphical-Models/">机器学习(三): 有向图模型 (Directed Graphical Models)</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2020/08/24/t-SNE/">t-SNE 高维数据可视化 (t-SNE: High-Dimensional Data Visualization)</a></p>
        
    </div>
</div>


        <!-- <h2 id="comments">Comments</h2>
        



 -->


    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right-jekyll">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id]):not([class])')
    for (var i = 0; i < aTags.length; i++) {
        if (aTags[i].getAttribute('href').startsWith('#'))
        {
            continue
        }
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>

    <footer class="site-footer">


    <div class="wrapper">

        <p class="contact">
            联系我: 
            <a href="https://github.com/jarvis73" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:zjw.math@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>   
            <a href="https://www.zhihu.com/people/lin-xi-1-1" title="Zhihu"><i class="iconfont icon-daoruzhihu"></i></a>      
        </p>
        <p>
            <i class="fa fa-eye" style="padding-right: 2px;"></i> 访问量: <span id="busuanzi_value_site_pv"></span>次 | <i class="fa fa-user" style="padding-right: 2px;"></i> 访客数<span id="busuanzi_value_site_uv"></span>人次
        </p>
        <p class="power">
            网站支持 <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a> | 主题支持 <a href="https://github.com/Gaohaoyang/gaohaoyang.github.io">HyG</a> & <a href="https://github.com/Jarvis73/jarvis73.github.io">Jarvis73</a>
        </p>
        <p>
            <img src="/images/misc/beian.png" style="padding-right: 2px; padding-bottom: 4px; vertical-align: middle;" /> <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo" class="beian" >浙公网安备 33010602011353号 | </a>
            <a target="_blank" href="https://beian.miit.gov.cn/" class="beian">浙ICP备2020038513号-1</a>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script type='text/javascript' src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>
    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/lib/jquery/jquery.js" charset="utf-8"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/zui/1.9.2/js/zui.min.js" charset="utf-8"></script>
    <script src="/js/index_page.js" charset="utf-8"></script>
    <script src="/js/functions.js" charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
  </body>

</html>
